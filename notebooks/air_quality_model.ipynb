{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features for bengaluru train set\n",
      "Creating lag features for bengaluru val set\n",
      "Creating lag features for bengaluru test set\n",
      "\n",
      "Bengaluru data loaded:\n",
      "Train: 1241 samples\n",
      "Validation: 287 samples\n",
      "Test: 382 samples\n",
      "Creating lag features for chennai train set\n",
      "Creating lag features for chennai val set\n",
      "Creating lag features for chennai test set\n",
      "\n",
      "Chennai data loaded:\n",
      "Train: 1224 samples\n",
      "Validation: 283 samples\n",
      "Test: 377 samples\n",
      "Creating lag features for delhi train set\n",
      "Creating lag features for delhi val set\n",
      "Creating lag features for delhi test set\n",
      "\n",
      "Delhi data loaded:\n",
      "Train: 1299 samples\n",
      "Validation: 300 samples\n",
      "Test: 400 samples\n",
      "Creating lag features for hyderabad train set\n",
      "Creating lag features for hyderabad val set\n",
      "Creating lag features for hyderabad test set\n",
      "\n",
      "Hyderabad data loaded:\n",
      "Train: 1222 samples\n",
      "Validation: 282 samples\n",
      "Test: 376 samples\n"
     ]
    }
   ],
   "source": [
    "def create_lag_features(data, target_col='AQI', n_lags=7):\n",
    "    \"\"\"Create lag features for the target column\"\"\"\n",
    "    data = data.sort_values('Date')\n",
    "    for i in range(1, n_lags + 1):\n",
    "        data[f'{target_col}_lag_{i}'] = data[target_col].shift(i)\n",
    "    return data\n",
    "\n",
    "# Cell 2: Load pre-split data for each city\n",
    "processed_dir = '../data/processed'\n",
    "cities = ['bengaluru', 'chennai', 'delhi', 'hyderabad']\n",
    "\n",
    "# Dictionary to store data for each city\n",
    "city_data = {}\n",
    "\n",
    "for city in cities:\n",
    "    city_dir = f'{processed_dir}/{city.lower()}'\n",
    "    city_data[city] = {\n",
    "        'train': pd.read_csv(f'{city_dir}/train.csv'),\n",
    "        'val': pd.read_csv(f'{city_dir}/val.csv'),\n",
    "        'test': pd.read_csv(f'{city_dir}/test.csv')\n",
    "    }\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        city_data[city][split]['Date'] = pd.to_datetime(city_data[city][split]['Date'])\n",
    "        \n",
    "        # Check if lag features exist, if not create them\n",
    "        if 'AQI_lag_1' not in city_data[city][split].columns:\n",
    "            print(f\"Creating lag features for {city} {split} set\")\n",
    "            city_data[city][split] = create_lag_features(city_data[city][split])\n",
    "    \n",
    "    print(f\"\\n{city.title()} data loaded:\")\n",
    "    print(f\"Train: {city_data[city]['train'].shape[0]} samples\")\n",
    "    print(f\"Validation: {city_data[city]['val'].shape[0]} samples\")\n",
    "    print(f\"Test: {city_data[city]['test'].shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define utility functions\n",
    "def calculate_metrics(y_true, y_pred, prefix=''):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics with confidence intervals\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Remove any NaN values from both arrays\n",
    "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    \n",
    "    if len(y_true) == 0 or len(y_pred) == 0:\n",
    "        print(\"Warning: No valid data points after removing NaN values\")\n",
    "        return {f'{prefix}rmse': np.nan, f'{prefix}mae': np.nan, f'{prefix}r2': np.nan}\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    metrics[f'{prefix}rmse'] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    metrics[f'{prefix}mae'] = mean_absolute_error(y_true, y_pred)\n",
    "    metrics[f'{prefix}r2'] = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate confidence intervals using bootstrap\n",
    "    n_iterations = 1000\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    # Initialize arrays to store bootstrap metrics\n",
    "    rmse_boots = np.zeros(n_iterations)\n",
    "    mae_boots = np.zeros(n_iterations)\n",
    "    r2_boots = np.zeros(n_iterations)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Sample with replacement\n",
    "        indices = np.random.randint(0, n_samples, n_samples)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_pred_boot = y_pred[indices]\n",
    "        \n",
    "        # Calculate metrics for this bootstrap sample\n",
    "        rmse_boots[i] = np.sqrt(mean_squared_error(y_true_boot, y_pred_boot))\n",
    "        mae_boots[i] = mean_absolute_error(y_true_boot, y_pred_boot)\n",
    "        r2_boots[i] = r2_score(y_true_boot, y_pred_boot)\n",
    "    \n",
    "    # Calculate 95% confidence intervals\n",
    "    for metric_name, metric_boots in [('rmse', rmse_boots), ('mae', mae_boots), ('r2', r2_boots)]:\n",
    "        lower, upper = np.percentile(metric_boots, [2.5, 97.5])\n",
    "        metrics[f'{prefix}{metric_name}_ci'] = (lower, upper)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print metrics in a formatted way\"\"\"\n",
    "    for metric_name, value in metrics.items():\n",
    "        if '_ci' in metric_name:\n",
    "            print(f\"{metric_name}: ({value[0]:.3f}, {value[1]:.3f})\")\n",
    "        else:\n",
    "            print(f\"{metric_name}: {value:.3f}\")\n",
    "\n",
    "def plot_metrics_comparison(city, results):\n",
    "    \"\"\"Plot comparison of metrics for different models\"\"\"\n",
    "    models = ['persistence', 'moving_average']\n",
    "    metrics = ['rmse', 'mae', 'r2']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        # Get values and confidence intervals\n",
    "        values = []\n",
    "        cis = []\n",
    "        for model in models:\n",
    "            val_metric = results[model]['validation'][f'val_{metric}']\n",
    "            val_ci = results[model]['validation'][f'val_{metric}_ci']\n",
    "            values.append(val_metric)\n",
    "            cis.append(val_ci)\n",
    "        \n",
    "        # Plot bars with error bars\n",
    "        x = np.arange(len(models))\n",
    "        axes[idx].bar(x, values, yerr=[(v-ci[0], ci[1]-v) for v, ci in zip(values, cis)], \n",
    "                     capsize=5, alpha=0.7)\n",
    "        \n",
    "        axes[idx].set_xticks(x)\n",
    "        axes[idx].set_xticklabels([m.replace('_', ' ').title() for m in models])\n",
    "        axes[idx].set_title(f'{metric.upper()} Comparison')\n",
    "        axes[idx].set_ylabel(metric.upper())\n",
    "    \n",
    "    plt.suptitle(f'Model Performance Comparison - {city.title()}', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_predictions(city, results, data_split='test'):\n",
    "    \"\"\"Plot predictions from baseline models\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Get actual values\n",
    "    actual = city_data[city][data_split]['AQI']\n",
    "    dates = city_data[city][data_split]['Date']\n",
    "    \n",
    "    # Plot actual values\n",
    "    plt.plot(dates, actual, label='Actual', color='black', alpha=0.6)\n",
    "    \n",
    "    # Plot predictions for each model\n",
    "    for model_name, color in [('persistence', 'blue'), ('moving_average', 'red')]:\n",
    "        predictions = results[model_name]['predictions'][data_split]\n",
    "        plt.plot(dates, predictions, \n",
    "                label=model_name.replace('_', ' ').title(),\n",
    "                color=color, alpha=0.6)\n",
    "    \n",
    "    plt.title(f'{city.title()} - {data_split.title()} Set Predictions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('AQI')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Define baseline models\n",
    "class PersistenceModel:\n",
    "    def __init__(self):\n",
    "        self.name = \"Persistence\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # No training needed for persistence model\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # For persistence model, we'll use the 1-day lag if it exists\n",
    "        if 'AQI_lag_1' not in X.columns:\n",
    "            raise ValueError(\"AQI_lag_1 feature not found in the data\")\n",
    "        return X['AQI_lag_1'].values\n",
    "\n",
    "class MovingAverageModel:\n",
    "    def __init__(self, window_size=7):\n",
    "        self.name = \"Moving Average\"\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # No training needed for moving average model\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Create a rolling window of lag features\n",
    "        lag_cols = [f'AQI_lag_{i}' for i in range(1, self.window_size + 1)]\n",
    "        if not all(col in X.columns for col in lag_cols):\n",
    "            raise ValueError(f\"Required lag features not found in the data\")\n",
    "        \n",
    "        # Calculate moving average\n",
    "        lag_values = X[lag_cols].values\n",
    "        return np.nanmean(lag_values, axis=1)  # Use nanmean to handle NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Function to evaluate baseline models\n",
    "def evaluate_baseline_models(city_data, city_name):\n",
    "    \"\"\"Evaluate baseline models for a specific city\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize models\n",
    "    persistence = PersistenceModel()\n",
    "    moving_avg = MovingAverageModel(window_size=7)\n",
    "    \n",
    "    # Fit and evaluate persistence model\n",
    "    persistence.fit(city_data['train'], city_data['train']['AQI'])\n",
    "    val_pred_persistence = persistence.predict(city_data['val'])\n",
    "    test_pred_persistence = persistence.predict(city_data['test'])\n",
    "    \n",
    "    # Fit and evaluate moving average model\n",
    "    moving_avg.fit(city_data['train'], city_data['train']['AQI'])\n",
    "    val_pred_ma = moving_avg.predict(city_data['val'])\n",
    "    test_pred_ma = moving_avg.predict(city_data['test'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    persistence_val_metrics = calculate_metrics(city_data['val']['AQI'].values, val_pred_persistence, 'val_')\n",
    "    persistence_test_metrics = calculate_metrics(city_data['test']['AQI'].values, test_pred_persistence, 'test_')\n",
    "    \n",
    "    ma_val_metrics = calculate_metrics(city_data['val']['AQI'].values, val_pred_ma, 'val_')\n",
    "    ma_test_metrics = calculate_metrics(city_data['test']['AQI'].values, test_pred_ma, 'test_')\n",
    "    \n",
    "    # Store results\n",
    "    results['persistence'] = {\n",
    "        'validation': persistence_val_metrics,\n",
    "        'test': persistence_test_metrics,\n",
    "        'predictions': {\n",
    "            'val': val_pred_persistence,\n",
    "            'test': test_pred_persistence\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results['moving_average'] = {\n",
    "        'validation': ma_val_metrics,\n",
    "        'test': ma_test_metrics,\n",
    "        'predictions': {\n",
    "            'val': val_pred_ma,\n",
    "            'test': test_pred_ma\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define ModelResultsManager class\n",
    "class ModelResultsManager:\n",
    "    def __init__(self, base_dir='results'):\n",
    "        self.base_dir = base_dir\n",
    "        self._create_directory_structure()\n",
    "    \n",
    "    def _create_directory_structure(self):\n",
    "        \"\"\"Create necessary directories for storing results\"\"\"\n",
    "        os.makedirs(f'{self.base_dir}/baseline', exist_ok=True)\n",
    "        os.makedirs(f'{self.base_dir}/model_configs', exist_ok=True)\n",
    "        os.makedirs(f'{self.base_dir}/predictions/baseline', exist_ok=True)\n",
    "        os.makedirs(f'{self.base_dir}/performance_metrics', exist_ok=True)\n",
    "        os.makedirs(f'{self.base_dir}/plots/baseline', exist_ok=True)\n",
    "    \n",
    "    def _update_metadata(self, city_name, model_type, results):\n",
    "        \"\"\"Update metadata file with latest results\"\"\"\n",
    "        metadata_file = f'{self.base_dir}/metadata.json'\n",
    "        \n",
    "        # Load existing metadata or create new\n",
    "        if os.path.exists(metadata_file):\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "        else:\n",
    "            metadata = {}\n",
    "        \n",
    "        # Update metadata for this city and model type\n",
    "        if city_name not in metadata:\n",
    "            metadata[city_name] = {}\n",
    "        \n",
    "        metadata[city_name][model_type] = {\n",
    "            'last_updated': datetime.now().isoformat(),\n",
    "            'metrics_used': ['rmse', 'mae', 'r2'],\n",
    "            'results': {\n",
    "                'persistence': {\n",
    "                    'validation': results['persistence']['validation'],\n",
    "                    'test': results['persistence']['test']\n",
    "                },\n",
    "                'moving_average': {\n",
    "                    'validation': results['moving_average']['validation'],\n",
    "                    'test': results['moving_average']['test']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save updated metadata\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "    \n",
    "    def _convert_to_serializable(self, obj):\n",
    "        \"\"\"Convert numpy arrays and other non-serializable objects to serializable format\"\"\"\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: self._convert_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_to_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(self._convert_to_serializable(item) for item in obj)\n",
    "        elif isinstance(obj, (np.int64, np.int32, np.int16, np.int8,\n",
    "                            np.uint64, np.uint32, np.uint16, np.uint8)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float64, np.float32, np.float16)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return obj\n",
    "    \n",
    "    def save_baseline_results(self, city_name, results):\n",
    "        \"\"\"Save baseline model results for a city\"\"\"\n",
    "        # Convert results to serializable format\n",
    "        serializable_results = self._convert_to_serializable(results)\n",
    "        \n",
    "        # Save performance metrics\n",
    "        metrics_file = f'{self.base_dir}/performance_metrics/baseline_{city_name}.json'\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(serializable_results, f, indent=4)\n",
    "        \n",
    "        # Save predictions\n",
    "        pred_dir = f'{self.base_dir}/predictions/baseline/{city_name}'\n",
    "        os.makedirs(pred_dir, exist_ok=True)\n",
    "        \n",
    "        for model_name in ['persistence', 'moving_average']:\n",
    "            pred_data = {\n",
    "                'validation': serializable_results[model_name]['predictions']['val'],\n",
    "                'test': serializable_results[model_name]['predictions']['test']\n",
    "            }\n",
    "            with open(f'{pred_dir}/{model_name}_predictions.json', 'w') as f:\n",
    "                json.dump(pred_data, f, indent=4)\n",
    "        \n",
    "        # Update metadata\n",
    "        self._update_metadata(city_name, 'baseline', serializable_results)\n",
    "    \n",
    "    def save_plots(self, city_name, fig, plot_name):\n",
    "        \"\"\"Save plots for a city\"\"\"\n",
    "        # Create directory for city plots if it doesn't exist\n",
    "        plot_dir = f'{self.base_dir}/plots/baseline/{city_name}'\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = f'{plot_dir}/{plot_name}.png'\n",
    "        fig.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "    \n",
    "    def load_baseline_results(self, city_name):\n",
    "        \"\"\"Load baseline model results for a city\"\"\"\n",
    "        metrics_file = f'{self.base_dir}/performance_metrics/baseline_{city_name}.json'\n",
    "        if not os.path.exists(metrics_file):\n",
    "            raise FileNotFoundError(f\"No results found for {city_name}\")\n",
    "        \n",
    "        with open(metrics_file, 'r') as f:\n",
    "            return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating baseline models for bengaluru\n",
      "\n",
      "Persistence Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 17.738\n",
      "val_mae: 12.563\n",
      "val_r2: 0.512\n",
      "val_rmse_ci: (15.444, 20.144)\n",
      "val_mae_ci: (11.185, 14.067)\n",
      "val_r2_ci: (0.358, 0.629)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 11.324\n",
      "test_mae: 7.906\n",
      "test_r2: 0.667\n",
      "test_rmse_ci: (9.908, 12.823)\n",
      "test_mae_ci: (7.149, 8.745)\n",
      "test_r2_ci: (0.561, 0.745)\n",
      "\n",
      "Moving Average Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 21.624\n",
      "val_mae: 16.289\n",
      "val_r2: 0.275\n",
      "val_rmse_ci: (19.373, 24.066)\n",
      "val_mae_ci: (14.733, 18.027)\n",
      "val_r2_ci: (0.134, 0.388)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 13.973\n",
      "test_mae: 9.713\n",
      "test_r2: 0.492\n",
      "test_rmse_ci: (12.062, 15.925)\n",
      "test_mae_ci: (8.771, 10.733)\n",
      "test_r2_ci: (0.384, 0.580)\n",
      "\n",
      "Results and plots saved for bengaluru\n",
      "\n",
      "Evaluating baseline models for chennai\n",
      "\n",
      "Persistence Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 36.885\n",
      "val_mae: 23.734\n",
      "val_r2: 0.269\n",
      "val_rmse_ci: (31.742, 41.922)\n",
      "val_mae_ci: (20.489, 27.078)\n",
      "val_r2_ci: (0.052, 0.447)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 27.243\n",
      "test_mae: 16.415\n",
      "test_r2: 0.426\n",
      "test_rmse_ci: (22.675, 31.986)\n",
      "test_mae_ci: (14.319, 18.644)\n",
      "test_r2_ci: (0.190, 0.596)\n",
      "\n",
      "Moving Average Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 43.660\n",
      "val_mae: 31.500\n",
      "val_r2: -0.025\n",
      "val_rmse_ci: (38.548, 48.607)\n",
      "val_mae_ci: (27.977, 34.840)\n",
      "val_r2_ci: (-0.184, 0.119)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 32.677\n",
      "test_mae: 19.959\n",
      "test_r2: 0.174\n",
      "test_rmse_ci: (27.015, 38.921)\n",
      "test_mae_ci: (17.509, 22.664)\n",
      "test_r2_ci: (-0.110, 0.374)\n",
      "\n",
      "Results and plots saved for chennai\n",
      "\n",
      "Evaluating baseline models for delhi\n",
      "\n",
      "Persistence Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 53.260\n",
      "val_mae: 39.077\n",
      "val_r2: 0.792\n",
      "val_rmse_ci: (47.223, 59.288)\n",
      "val_mae_ci: (34.972, 43.258)\n",
      "val_r2_ci: (0.733, 0.841)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 45.105\n",
      "test_mae: 31.559\n",
      "test_r2: 0.843\n",
      "test_rmse_ci: (40.554, 49.719)\n",
      "test_mae_ci: (28.403, 34.853)\n",
      "test_r2_ci: (0.805, 0.872)\n",
      "\n",
      "Moving Average Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 65.436\n",
      "val_mae: 50.511\n",
      "val_r2: 0.686\n",
      "val_rmse_ci: (59.405, 71.751)\n",
      "val_mae_ci: (45.956, 55.136)\n",
      "val_r2_ci: (0.607, 0.747)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 66.572\n",
      "test_mae: 46.647\n",
      "test_r2: 0.658\n",
      "test_rmse_ci: (60.683, 72.621)\n",
      "test_mae_ci: (42.384, 51.291)\n",
      "test_r2_ci: (0.579, 0.716)\n",
      "\n",
      "Results and plots saved for delhi\n",
      "\n",
      "Evaluating baseline models for hyderabad\n",
      "\n",
      "Persistence Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 14.052\n",
      "val_mae: 10.559\n",
      "val_r2: 0.719\n",
      "val_rmse_ci: (12.357, 15.961)\n",
      "val_mae_ci: (9.523, 11.644)\n",
      "val_r2_ci: (0.624, 0.792)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 14.221\n",
      "test_mae: 9.861\n",
      "test_r2: 0.840\n",
      "test_rmse_ci: (12.460, 15.848)\n",
      "test_mae_ci: (8.901, 10.835)\n",
      "test_r2_ci: (0.796, 0.878)\n",
      "\n",
      "Moving Average Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 21.699\n",
      "val_mae: 16.349\n",
      "val_r2: 0.331\n",
      "val_rmse_ci: (19.371, 23.912)\n",
      "val_mae_ci: (14.703, 17.961)\n",
      "val_r2_ci: (0.186, 0.453)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 21.981\n",
      "test_mae: 16.111\n",
      "test_r2: 0.619\n",
      "test_rmse_ci: (19.849, 24.189)\n",
      "test_mae_ci: (14.599, 17.655)\n",
      "test_r2_ci: (0.536, 0.686)\n",
      "\n",
      "Results and plots saved for hyderabad\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Run evaluation and save results\n",
    "results_manager = ModelResultsManager()\n",
    "baseline_results = {}\n",
    "\n",
    "for city in cities:\n",
    "    print(f\"\\nEvaluating baseline models for {city}\")\n",
    "    baseline_results[city] = evaluate_baseline_models(city_data[city], city)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nPersistence Model:\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    print_metrics(baseline_results[city]['persistence']['validation'])\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print_metrics(baseline_results[city]['persistence']['test'])\n",
    "    \n",
    "    print(\"\\nMoving Average Model:\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    print_metrics(baseline_results[city]['moving_average']['validation'])\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print_metrics(baseline_results[city]['moving_average']['test'])\n",
    "    \n",
    "    # Save results\n",
    "    results_manager.save_baseline_results(city, baseline_results[city])\n",
    "    \n",
    "    # Generate and save plots\n",
    "    fig = plot_metrics_comparison(city, baseline_results[city])\n",
    "    results_manager.save_plots(city, fig, 'metrics_comparison')\n",
    "    \n",
    "    fig = plot_predictions(city, baseline_results[city], 'test')\n",
    "    results_manager.save_plots(city, fig, 'test_predictions')\n",
    "    \n",
    "    print(f\"\\nResults and plots saved for {city}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

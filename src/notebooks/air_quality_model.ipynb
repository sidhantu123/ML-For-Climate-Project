{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "\n",
    "# # Set visualization style\n",
    "# plt.style.use('seaborn')\n",
    "# sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features for bengaluru train set\n",
      "Creating lag features for bengaluru val set\n",
      "Creating lag features for bengaluru test set\n",
      "\n",
      "Bengaluru data loaded:\n",
      "Train: 1241 samples\n",
      "Validation: 287 samples\n",
      "Test: 382 samples\n",
      "Creating lag features for chennai train set\n",
      "Creating lag features for chennai val set\n",
      "Creating lag features for chennai test set\n",
      "\n",
      "Chennai data loaded:\n",
      "Train: 1224 samples\n",
      "Validation: 283 samples\n",
      "Test: 377 samples\n",
      "Creating lag features for delhi train set\n",
      "Creating lag features for delhi val set\n",
      "Creating lag features for delhi test set\n",
      "\n",
      "Delhi data loaded:\n",
      "Train: 1299 samples\n",
      "Validation: 300 samples\n",
      "Test: 400 samples\n",
      "Creating lag features for hyderabad train set\n",
      "Creating lag features for hyderabad val set\n",
      "Creating lag features for hyderabad test set\n",
      "\n",
      "Hyderabad data loaded:\n",
      "Train: 1222 samples\n",
      "Validation: 282 samples\n",
      "Test: 376 samples\n"
     ]
    }
   ],
   "source": [
    "def create_lag_features(data, target_col='AQI', n_lags=7):\n",
    "    \"\"\"Create lag features for the target column\"\"\"\n",
    "    data = data.sort_values('Date')\n",
    "    for i in range(1, n_lags + 1):\n",
    "        data[f'{target_col}_lag_{i}'] = data[target_col].shift(i)\n",
    "    return data\n",
    "\n",
    "# Cell 2: Load pre-split data for each city\n",
    "processed_dir = '../data/processed'\n",
    "cities = ['bengaluru', 'chennai', 'delhi', 'hyderabad']\n",
    "\n",
    "# Dictionary to store data for each city\n",
    "city_data = {}\n",
    "\n",
    "for city in cities:\n",
    "    city_dir = f'{processed_dir}/{city.lower()}'\n",
    "    city_data[city] = {\n",
    "        'train': pd.read_csv(f'{city_dir}/train.csv'),\n",
    "        'val': pd.read_csv(f'{city_dir}/val.csv'),\n",
    "        'test': pd.read_csv(f'{city_dir}/test.csv')\n",
    "    }\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        city_data[city][split]['Date'] = pd.to_datetime(city_data[city][split]['Date'])\n",
    "        \n",
    "        # Check if lag features exist, if not create them\n",
    "        if 'AQI_lag_1' not in city_data[city][split].columns:\n",
    "            print(f\"Creating lag features for {city} {split} set\")\n",
    "            city_data[city][split] = create_lag_features(city_data[city][split])\n",
    "    \n",
    "    print(f\"\\n{city.title()} data loaded:\")\n",
    "    print(f\"Train: {city_data[city]['train'].shape[0]} samples\")\n",
    "    print(f\"Validation: {city_data[city]['val'].shape[0]} samples\")\n",
    "    print(f\"Test: {city_data[city]['test'].shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define utility functions\n",
    "def calculate_metrics(y_true, y_pred, prefix=''):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics with confidence intervals\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Remove any NaN values from both arrays\n",
    "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    \n",
    "    if len(y_true) == 0 or len(y_pred) == 0:\n",
    "        print(\"Warning: No valid data points after removing NaN values\")\n",
    "        return {f'{prefix}rmse': np.nan, f'{prefix}mae': np.nan, f'{prefix}r2': np.nan}\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    metrics[f'{prefix}rmse'] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    metrics[f'{prefix}mae'] = mean_absolute_error(y_true, y_pred)\n",
    "    metrics[f'{prefix}r2'] = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate confidence intervals using bootstrap\n",
    "    n_iterations = 1000\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    # Initialize arrays to store bootstrap metrics\n",
    "    rmse_boots = np.zeros(n_iterations)\n",
    "    mae_boots = np.zeros(n_iterations)\n",
    "    r2_boots = np.zeros(n_iterations)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Sample with replacement\n",
    "        indices = np.random.randint(0, n_samples, n_samples)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_pred_boot = y_pred[indices]\n",
    "        \n",
    "        # Calculate metrics for this bootstrap sample\n",
    "        rmse_boots[i] = np.sqrt(mean_squared_error(y_true_boot, y_pred_boot))\n",
    "        mae_boots[i] = mean_absolute_error(y_true_boot, y_pred_boot)\n",
    "        r2_boots[i] = r2_score(y_true_boot, y_pred_boot)\n",
    "    \n",
    "    # Calculate 95% confidence intervals\n",
    "    for metric_name, metric_boots in [('rmse', rmse_boots), ('mae', mae_boots), ('r2', r2_boots)]:\n",
    "        lower, upper = np.percentile(metric_boots, [2.5, 97.5])\n",
    "        metrics[f'{prefix}{metric_name}_ci'] = (lower, upper)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print metrics in a formatted way\"\"\"\n",
    "    if isinstance(metrics, tuple):\n",
    "        # Handle confidence intervals\n",
    "        print(f\"({metrics[0]:.3f}, {metrics[1]:.3f})\")\n",
    "    elif isinstance(metrics, dict):\n",
    "        # Handle metric dictionaries\n",
    "        for metric_name, value in metrics.items():\n",
    "            if isinstance(value, tuple):\n",
    "                print(f\"{metric_name}: ({value[0]:.3f}, {value[1]:.3f})\")\n",
    "            else:\n",
    "                print(f\"{metric_name}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"Unknown metric type: {type(metrics)}\")\n",
    "\n",
    "def plot_metrics_comparison(city, results):\n",
    "    \"\"\"Plot comparison of metrics for different models\"\"\"\n",
    "    models = ['persistence', 'moving_average']\n",
    "    metrics = ['rmse', 'mae', 'r2']\n",
    "    splits = ['validation', 'test']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        # Get values and confidence intervals for each split\n",
    "        values = {split: [] for split in splits}\n",
    "        cis = {split: [] for split in splits}\n",
    "        \n",
    "        for model in models:\n",
    "            for split in splits:\n",
    "                metric_key = f'{split[:3]}_{metric}'\n",
    "                ci_key = f'{split[:3]}_{metric}_ci'\n",
    "                \n",
    "                if split in results[model] and metric_key in results[model][split]:\n",
    "                    values[split].append(results[model][split][metric_key])\n",
    "                    cis[split].append(results[model][split][ci_key])\n",
    "        \n",
    "        # Only plot if we have data\n",
    "        if any(values[split] for split in splits):\n",
    "            # Plot bars with error bars for each split\n",
    "            x = np.arange(len(models))\n",
    "            width = 0.35\n",
    "            \n",
    "            for i, split in enumerate(splits):\n",
    "                if values[split]:  # Only plot if we have values\n",
    "                    axes[idx].bar(x + i*width, values[split], width,\n",
    "                                yerr=[(v-ci[0], ci[1]-v) for v, ci in zip(values[split], cis[split])],\n",
    "                                label=split.title(), capsize=5, alpha=0.7)\n",
    "            \n",
    "            axes[idx].set_xticks(x + width/2)\n",
    "            axes[idx].set_xticklabels([m.replace('_', ' ').title() for m in models])\n",
    "            axes[idx].set_title(f'{metric.upper()} Comparison')\n",
    "            axes[idx].set_ylabel(metric.upper())\n",
    "            axes[idx].legend()\n",
    "        else:\n",
    "            # If no data, show a message\n",
    "            axes[idx].text(0.5, 0.5, 'No data available',\n",
    "                         horizontalalignment='center',\n",
    "                         verticalalignment='center',\n",
    "                         transform=axes[idx].transAxes)\n",
    "            axes[idx].set_title(f'{metric.upper()} Comparison')\n",
    "    \n",
    "    plt.suptitle(f'Model Performance Comparison - {city.title()}', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_predictions(city, results, data_split='test'):\n",
    "    \"\"\"Plot predictions from baseline models\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Get actual values\n",
    "    actual = city_data[city][data_split]['AQI']\n",
    "    dates = city_data[city][data_split]['Date']\n",
    "    \n",
    "    # Plot actual values\n",
    "    plt.plot(dates, actual, label='Actual', color='black', alpha=0.6)\n",
    "    \n",
    "    # Plot predictions for each model\n",
    "    for model_name, color in [('persistence', 'blue'), ('moving_average', 'red')]:\n",
    "        predictions = results[model_name]['predictions'][data_split]\n",
    "        plt.plot(dates, predictions, \n",
    "                label=model_name.replace('_', ' ').title(),\n",
    "                color=color, alpha=0.6)\n",
    "    \n",
    "    plt.title(f'{city.title()} - {data_split.title()} Set Predictions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('AQI')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Define baseline models\n",
    "class PersistenceModel:\n",
    "    def __init__(self):\n",
    "        self.name = \"Persistence\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # No training needed for persistence model\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # For persistence model, we'll use the 1-day lag if it exists\n",
    "        if 'AQI_lag_1' not in X.columns:\n",
    "            raise ValueError(\"AQI_lag_1 feature not found in the data\")\n",
    "        return X['AQI_lag_1'].values\n",
    "\n",
    "class MovingAverageModel:\n",
    "    def __init__(self, window_size=7):\n",
    "        self.name = \"Moving Average\"\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # No training needed for moving average model\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Create a rolling window of lag features\n",
    "        lag_cols = [f'AQI_lag_{i}' for i in range(1, self.window_size + 1)]\n",
    "        if not all(col in X.columns for col in lag_cols):\n",
    "            raise ValueError(f\"Required lag features not found in the data\")\n",
    "        \n",
    "        # Calculate moving average\n",
    "        lag_values = X[lag_cols].values\n",
    "        return np.nanmean(lag_values, axis=1)  # Use nanmean to handle NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Function to evaluate baseline models\n",
    "def evaluate_baseline_models(city_data, city_name):\n",
    "    \"\"\"Evaluate baseline models for a specific city\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize models\n",
    "    persistence = PersistenceModel()\n",
    "    moving_avg = MovingAverageModel(window_size=7)\n",
    "    \n",
    "    # Fit and evaluate persistence model\n",
    "    persistence.fit(city_data['train'], city_data['train']['AQI'])\n",
    "    val_pred_persistence = persistence.predict(city_data['val'])\n",
    "    test_pred_persistence = persistence.predict(city_data['test'])\n",
    "    \n",
    "    # Fit and evaluate moving average model\n",
    "    moving_avg.fit(city_data['train'], city_data['train']['AQI'])\n",
    "    val_pred_ma = moving_avg.predict(city_data['val'])\n",
    "    test_pred_ma = moving_avg.predict(city_data['test'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    persistence_val_metrics = calculate_metrics(city_data['val']['AQI'].values, val_pred_persistence, 'val_')\n",
    "    persistence_test_metrics = calculate_metrics(city_data['test']['AQI'].values, test_pred_persistence, 'test_')\n",
    "    \n",
    "    ma_val_metrics = calculate_metrics(city_data['val']['AQI'].values, val_pred_ma, 'val_')\n",
    "    ma_test_metrics = calculate_metrics(city_data['test']['AQI'].values, test_pred_ma, 'test_')\n",
    "    \n",
    "    # Store results\n",
    "    results['persistence'] = {\n",
    "        'validation': persistence_val_metrics,\n",
    "        'test': persistence_test_metrics,\n",
    "        'predictions': {\n",
    "            'val': val_pred_persistence,\n",
    "            'test': test_pred_persistence\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results['moving_average'] = {\n",
    "        'validation': ma_val_metrics,\n",
    "        'test': ma_test_metrics,\n",
    "        'predictions': {\n",
    "            'val': val_pred_ma,\n",
    "            'test': test_pred_ma\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define ModelResultsManager class\n",
    "class ModelResultsManager:\n",
    "    def __init__(self, base_dir='results'):\n",
    "        self.base_dir = base_dir\n",
    "        self._create_directory_structure()\n",
    "    \n",
    "    def _create_directory_structure(self):\n",
    "        \"\"\"Create necessary directories for storing results\"\"\"\n",
    "        os.makedirs(f'{self.base_dir}/baseline', exist_ok=True)\n",
    "        os.makedirs(f'{self.base_dir}/model_configs', exist_ok=True)\n",
    "        os.makedirs(f'{self.base_dir}/predictions/baseline', exist_ok=True)\n",
    "        os.makedirs(f'{self.base_dir}/performance_metrics', exist_ok=True)\n",
    "        os.makedirs(f'{self.base_dir}/plots/baseline', exist_ok=True)\n",
    "    \n",
    "    def _convert_to_serializable(self, obj):\n",
    "        \"\"\"Convert numpy arrays and other non-serializable objects to serializable format\"\"\"\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: self._convert_to_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_to_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(self._convert_to_serializable(item) for item in obj)\n",
    "        elif isinstance(obj, (np.int64, np.int32, np.int16, np.int8,\n",
    "                            np.uint64, np.uint32, np.uint16, np.uint8)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float64, np.float32, np.float16)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return obj\n",
    "    \n",
    "    def _update_metadata(self, city_name, model_type, results):\n",
    "        \"\"\"Update metadata file with latest results\"\"\"\n",
    "        metadata_file = f'{self.base_dir}/metadata.json'\n",
    "        \n",
    "        # Load existing metadata or create new\n",
    "        if os.path.exists(metadata_file):\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "        else:\n",
    "            metadata = {}\n",
    "        \n",
    "        # Update metadata for this city and model type\n",
    "        if city_name not in metadata:\n",
    "            metadata[city_name] = {}\n",
    "        \n",
    "        metadata[city_name][model_type] = {\n",
    "            'last_updated': datetime.now().isoformat(),\n",
    "            'metrics_used': ['rmse', 'mae', 'r2'],\n",
    "            'results': {\n",
    "                'persistence': {\n",
    "                    'validation': results['persistence']['validation'],\n",
    "                    'test': results['persistence']['test']\n",
    "                },\n",
    "                'moving_average': {\n",
    "                    'validation': results['moving_average']['validation'],\n",
    "                    'test': results['moving_average']['test']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save updated metadata\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "    \n",
    "    def save_baseline_results(self, city_name, results):\n",
    "        \"\"\"Save baseline model results for a city\"\"\"\n",
    "        # Convert results to serializable format\n",
    "        serializable_results = self._convert_to_serializable(results)\n",
    "        \n",
    "        # Save performance metrics\n",
    "        metrics_file = f'{self.base_dir}/performance_metrics/baseline_{city_name}.json'\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(serializable_results, f, indent=4)\n",
    "        \n",
    "        # Save predictions\n",
    "        pred_dir = f'{self.base_dir}/predictions/baseline/{city_name}'\n",
    "        os.makedirs(pred_dir, exist_ok=True)\n",
    "        \n",
    "        for model_name in ['persistence', 'moving_average']:\n",
    "            pred_data = {\n",
    "                'validation': serializable_results[model_name]['predictions']['val'],\n",
    "                'test': serializable_results[model_name]['predictions']['test']\n",
    "            }\n",
    "            with open(f'{pred_dir}/{model_name}_predictions.json', 'w') as f:\n",
    "                json.dump(pred_data, f, indent=4)\n",
    "        \n",
    "        # Update metadata\n",
    "        self._update_metadata(city_name, 'baseline', serializable_results)\n",
    "    \n",
    "    def save_plots(self, city_name, fig, plot_name):\n",
    "        \"\"\"Save plots for a city\"\"\"\n",
    "        # Create directory for city plots if it doesn't exist\n",
    "        plot_dir = f'{self.base_dir}/plots/baseline/{city_name}'\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = f'{plot_dir}/{plot_name}.png'\n",
    "        fig.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)  # Close the figure to free memory\n",
    "    \n",
    "    def load_baseline_results(self, city_name):\n",
    "        \"\"\"Load baseline model results for a city\"\"\"\n",
    "        metrics_file = f'{self.base_dir}/performance_metrics/baseline_{city_name}.json'\n",
    "        if not os.path.exists(metrics_file):\n",
    "            raise FileNotFoundError(f\"No results found for {city_name}\")\n",
    "        \n",
    "        with open(metrics_file, 'r') as f:\n",
    "            return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating baseline models for bengaluru\n",
      "\n",
      "Persistence Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 17.738\n",
      "val_mae: 12.563\n",
      "val_r2: 0.512\n",
      "val_rmse_ci: (15.359, 20.098)\n",
      "val_mae_ci: (11.066, 14.046)\n",
      "val_r2_ci: (0.348, 0.625)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 11.324\n",
      "test_mae: 7.906\n",
      "test_r2: 0.667\n",
      "test_rmse_ci: (9.855, 12.752)\n",
      "test_mae_ci: (7.105, 8.693)\n",
      "test_r2_ci: (0.572, 0.749)\n",
      "\n",
      "Moving Average Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 21.624\n",
      "val_mae: 16.289\n",
      "val_r2: 0.275\n",
      "val_rmse_ci: (19.316, 23.925)\n",
      "val_mae_ci: (14.764, 17.803)\n",
      "val_r2_ci: (0.127, 0.384)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 13.973\n",
      "test_mae: 9.713\n",
      "test_r2: 0.492\n",
      "test_rmse_ci: (12.095, 15.927)\n",
      "test_mae_ci: (8.781, 10.745)\n",
      "test_r2_ci: (0.396, 0.576)\n",
      "\n",
      "Results and plots saved for bengaluru\n",
      "\n",
      "Evaluating baseline models for chennai\n",
      "\n",
      "Persistence Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 36.885\n",
      "val_mae: 23.734\n",
      "val_r2: 0.269\n",
      "val_rmse_ci: (31.710, 42.046)\n",
      "val_mae_ci: (20.595, 27.082)\n",
      "val_r2_ci: (0.064, 0.440)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 27.243\n",
      "test_mae: 16.415\n",
      "test_r2: 0.426\n",
      "test_rmse_ci: (22.644, 31.626)\n",
      "test_mae_ci: (14.266, 18.533)\n",
      "test_r2_ci: (0.202, 0.611)\n",
      "\n",
      "Moving Average Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 43.660\n",
      "val_mae: 31.500\n",
      "val_r2: -0.025\n",
      "val_rmse_ci: (38.270, 49.276)\n",
      "val_mae_ci: (28.153, 35.291)\n",
      "val_r2_ci: (-0.183, 0.117)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 32.677\n",
      "test_mae: 19.959\n",
      "test_r2: 0.174\n",
      "test_rmse_ci: (27.090, 39.005)\n",
      "test_mae_ci: (17.539, 22.772)\n",
      "test_r2_ci: (-0.143, 0.371)\n",
      "\n",
      "Results and plots saved for chennai\n",
      "\n",
      "Evaluating baseline models for delhi\n",
      "\n",
      "Persistence Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 53.260\n",
      "val_mae: 39.077\n",
      "val_r2: 0.792\n",
      "val_rmse_ci: (47.216, 59.210)\n",
      "val_mae_ci: (34.782, 43.112)\n",
      "val_r2_ci: (0.734, 0.843)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 45.105\n",
      "test_mae: 31.559\n",
      "test_r2: 0.843\n",
      "test_rmse_ci: (40.819, 49.805)\n",
      "test_mae_ci: (28.736, 34.672)\n",
      "test_r2_ci: (0.807, 0.872)\n",
      "\n",
      "Moving Average Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 65.436\n",
      "val_mae: 50.511\n",
      "val_r2: 0.686\n",
      "val_rmse_ci: (59.748, 71.437)\n",
      "val_mae_ci: (46.056, 55.023)\n",
      "val_r2_ci: (0.609, 0.746)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 66.572\n",
      "test_mae: 46.647\n",
      "test_r2: 0.658\n",
      "test_rmse_ci: (60.206, 72.404)\n",
      "test_mae_ci: (42.184, 51.441)\n",
      "test_r2_ci: (0.580, 0.723)\n",
      "\n",
      "Results and plots saved for delhi\n",
      "\n",
      "Evaluating baseline models for hyderabad\n",
      "\n",
      "Persistence Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 14.052\n",
      "val_mae: 10.559\n",
      "val_r2: 0.719\n",
      "val_rmse_ci: (12.399, 16.072)\n",
      "val_mae_ci: (9.590, 11.719)\n",
      "val_r2_ci: (0.619, 0.790)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 14.221\n",
      "test_mae: 9.861\n",
      "test_r2: 0.840\n",
      "test_rmse_ci: (12.588, 16.002)\n",
      "test_mae_ci: (8.896, 10.893)\n",
      "test_r2_ci: (0.793, 0.878)\n",
      "\n",
      "Moving Average Model:\n",
      "Validation Metrics:\n",
      "val_rmse: 21.699\n",
      "val_mae: 16.349\n",
      "val_r2: 0.331\n",
      "val_rmse_ci: (19.473, 24.080)\n",
      "val_mae_ci: (14.648, 18.111)\n",
      "val_r2_ci: (0.175, 0.453)\n",
      "\n",
      "Test Metrics:\n",
      "test_rmse: 21.981\n",
      "test_mae: 16.111\n",
      "test_r2: 0.619\n",
      "test_rmse_ci: (19.926, 23.974)\n",
      "test_mae_ci: (14.679, 17.666)\n",
      "test_r2_ci: (0.538, 0.685)\n",
      "\n",
      "Results and plots saved for hyderabad\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Run evaluation and save results\n",
    "results_manager = ModelResultsManager()\n",
    "baseline_results = {}\n",
    "\n",
    "for city in cities:\n",
    "    print(f\"\\nEvaluating baseline models for {city}\")\n",
    "    baseline_results[city] = evaluate_baseline_models(city_data[city], city)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nPersistence Model:\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    print_metrics(baseline_results[city]['persistence']['validation'])\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print_metrics(baseline_results[city]['persistence']['test'])\n",
    "    \n",
    "    print(\"\\nMoving Average Model:\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    print_metrics(baseline_results[city]['moving_average']['validation'])\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print_metrics(baseline_results[city]['moving_average']['test'])\n",
    "    \n",
    "    # Save results\n",
    "    results_manager.save_baseline_results(city, baseline_results[city])\n",
    "    \n",
    "    # Generate and save plots\n",
    "    fig = plot_metrics_comparison(city, baseline_results[city])\n",
    "    results_manager.save_plots(city, fig, 'metrics_comparison')\n",
    "    \n",
    "    fig = plot_predictions(city, baseline_results[city], 'test')\n",
    "    results_manager.save_plots(city, fig, 'test_predictions')\n",
    "    \n",
    "    print(f\"\\nResults and plots saved for {city}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModelEvaluator:\n",
    "    def __init__(self, city_data, city_name):\n",
    "        self.city_data = city_data\n",
    "        self.city_name = city_name\n",
    "        self.scaler = StandardScaler()\n",
    "        self.results = {}\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare data for ML models\"\"\"\n",
    "        # Get numeric columns only (excluding date, city, and AQI)\n",
    "        numeric_cols = self.city_data['train'].select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in ['AQI']]\n",
    "        \n",
    "        # Separate features and target\n",
    "        X_train = self.city_data['train'][numeric_cols]\n",
    "        y_train = self.city_data['train']['AQI']\n",
    "        \n",
    "        X_val = self.city_data['val'][numeric_cols]\n",
    "        y_val = self.city_data['val']['AQI']\n",
    "        \n",
    "        X_test = self.city_data['test'][numeric_cols]\n",
    "        y_test = self.city_data['test']['AQI']\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_val_scaled = self.scaler.transform(X_val)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return {\n",
    "            'train': {'X': X_train_scaled, 'y': y_train},\n",
    "            'val': {'X': X_val_scaled, 'y': y_val},\n",
    "            'test': {'X': X_test_scaled, 'y': y_test},\n",
    "            'feature_names': numeric_cols  # Removed .tolist() since it's already a list\n",
    "        }\n",
    "    \n",
    "    def calculate_confidence_intervals(self, y_true, y_pred, metric_func, n_bootstrap=1000):\n",
    "        \"\"\"Calculate 95% confidence intervals using bootstrap sampling\"\"\"\n",
    "        scores = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            indices = np.random.choice(len(y_true), len(y_true), replace=True)\n",
    "            score = metric_func(y_true[indices], y_pred[indices])\n",
    "            scores.append(score)\n",
    "        return np.percentile(scores, [2.5, 97.5])\n",
    "    \n",
    "    def evaluate_model(self, model, model_name, param_grid=None):\n",
    "        \"\"\"Evaluate a single ML model\"\"\"\n",
    "        data = self.prepare_data()\n",
    "        \n",
    "        if param_grid:\n",
    "            grid_search = GridSearchCV(\n",
    "                model, param_grid, cv=5, scoring='neg_mean_squared_error',\n",
    "                n_jobs=-1, verbose=1\n",
    "            )\n",
    "            grid_search.fit(data['train']['X'], data['train']['y'])\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "        else:\n",
    "            best_model = model\n",
    "            best_model.fit(data['train']['X'], data['train']['y'])\n",
    "            best_params = {}\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = {\n",
    "            'val': best_model.predict(data['val']['X']),\n",
    "            'test': best_model.predict(data['test']['X'])\n",
    "        }\n",
    "        \n",
    "        # Calculate metrics with confidence intervals\n",
    "        metrics = {}\n",
    "        for split in ['val', 'test']:\n",
    "            y_true = data[split]['y']\n",
    "            y_pred = predictions[split]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            \n",
    "            # Calculate confidence intervals\n",
    "            rmse_ci = self.calculate_confidence_intervals(\n",
    "                y_true, y_pred, lambda yt, yp: np.sqrt(mean_squared_error(yt, yp))\n",
    "            )\n",
    "            mae_ci = self.calculate_confidence_intervals(\n",
    "                y_true, y_pred, mean_absolute_error\n",
    "            )\n",
    "            r2_ci = self.calculate_confidence_intervals(\n",
    "                y_true, y_pred, r2_score\n",
    "            )\n",
    "            \n",
    "            metrics[split] = {\n",
    "                f'{split}_rmse': rmse,\n",
    "                f'{split}_mae': mae,\n",
    "                f'{split}_r2': r2,\n",
    "                f'{split}_rmse_ci': rmse_ci.tolist(),\n",
    "                f'{split}_mae_ci': mae_ci.tolist(),\n",
    "                f'{split}_r2_ci': r2_ci.tolist()\n",
    "            }\n",
    "        \n",
    "        # Get feature importance if available\n",
    "        feature_importance = None\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            feature_importance = dict(zip(data['feature_names'], \n",
    "                                       best_model.feature_importances_))\n",
    "        elif hasattr(best_model, 'coef_'):\n",
    "            feature_importance = dict(zip(data['feature_names'], \n",
    "                                       best_model.coef_))\n",
    "        \n",
    "        # Store results in baseline-compatible format\n",
    "        self.results[model_name] = {\n",
    "            'val': metrics['val'],\n",
    "            'test': metrics['test'],\n",
    "            'predictions': {\n",
    "                'val': predictions['val'].tolist(),\n",
    "                'test': predictions['test'].tolist()\n",
    "            },\n",
    "            'model_config': {\n",
    "                'name': model_name,\n",
    "                'parameters': best_params,\n",
    "                'feature_importance': feature_importance\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return self.results[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_ml_models(city_data, city_name):\n",
    "    \"\"\"Evaluate all ML models for a given city\"\"\"\n",
    "    evaluator = MLModelEvaluator(city_data, city_name)\n",
    "    \n",
    "    # Define models and their parameter grids\n",
    "    models = {\n",
    "        'linear_regression': {\n",
    "            'model': LinearRegression(),\n",
    "            'param_grid': None\n",
    "        },\n",
    "        'ridge': {\n",
    "            'model': Ridge(),\n",
    "            'param_grid': {\n",
    "                'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "            }\n",
    "        },\n",
    "        'lasso': {\n",
    "            'model': Lasso(),\n",
    "            'param_grid': {\n",
    "                'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "            }\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'gradient_boosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'learning_rate': [0.01, 0.1],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'model': xgb.XGBRegressor(random_state=42),\n",
    "            'param_grid': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'learning_rate': [0.01, 0.1],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        },\n",
    "        'svr': {\n",
    "            'model': SVR(),\n",
    "            'param_grid': {\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'C': [0.1, 1.0, 10.0],\n",
    "                'epsilon': [0.1, 0.2]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_name, model_config in models.items():\n",
    "        print(f\"\\nEvaluating {model_name} for {city_name}...\")\n",
    "        evaluator.evaluate_model(\n",
    "            model_config['model'],\n",
    "            model_name,\n",
    "            model_config['param_grid']\n",
    "        )\n",
    "    \n",
    "    return evaluator.results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "def save_ml_results(city_name, results):\n",
    "    \"\"\"Save ML model results\"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    results_dir = f\"./results/ml_models/{city_name}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Save results for each model\n",
    "    for model_name, model_results in results.items():\n",
    "        # Save to file using the custom encoder\n",
    "        with open(f\"{results_dir}/{model_name}_results.json\", 'w') as f:\n",
    "            json.dump(model_results, f, indent=4, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(city, results):\n",
    "    \"\"\"Plot comparison of metrics for different models\"\"\"\n",
    "    # Define metrics and splits\n",
    "    metrics = ['rmse', 'mae', 'r2']\n",
    "    splits = ['val', 'test']\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    models = list(results.keys())\n",
    "    n_models = len(models)\n",
    "    n_metrics = len(metrics)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, n_metrics, figsize=(6*n_metrics, 6))\n",
    "    if n_metrics == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Collect values and confidence intervals\n",
    "        values = {split: [] for split in splits}\n",
    "        cis = {split: [] for split in splits}\n",
    "        \n",
    "        for model in models:\n",
    "            for split in splits:\n",
    "                metric_key = f'{split}_{metric}'\n",
    "                ci_key = f'{split}_{metric}_ci'\n",
    "                \n",
    "                if metric_key in results[model][split]:\n",
    "                    values[split].append(results[model][split][metric_key])\n",
    "                    # Convert CI to error bar format (half-width)\n",
    "                    ci = results[model][split][ci_key]\n",
    "                    if isinstance(ci, (list, np.ndarray)) and len(ci) == 2:\n",
    "                        ci_half_width = (ci[1] - ci[0]) / 2\n",
    "                        cis[split].append(ci_half_width)\n",
    "                    else:\n",
    "                        cis[split].append(0)  # No error bar if CI is invalid\n",
    "                else:\n",
    "                    values[split].append(None)\n",
    "                    cis[split].append(0)\n",
    "        \n",
    "        # Plot bars\n",
    "        x = np.arange(n_models)\n",
    "        width = 0.35\n",
    "        \n",
    "        for j, split in enumerate(splits):\n",
    "            valid_values = [v for v in values[split] if v is not None]\n",
    "            if valid_values:\n",
    "                ax.bar(x + j*width, values[split], width, label=split.capitalize())\n",
    "                \n",
    "                # Add error bars\n",
    "                for k, (val, ci) in enumerate(zip(values[split], cis[split])):\n",
    "                    if val is not None:\n",
    "                        ax.errorbar(x[k] + j*width, val, yerr=ci, \n",
    "                                  fmt='none', color='black', capsize=5)\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Models')\n",
    "        ax.set_ylabel(metric.upper())\n",
    "        ax.set_title(f'{metric.upper()} Comparison')\n",
    "        ax.set_xticks(x + width/2)\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_predictions(city, results, data_split, actual_values):\n",
    "    \"\"\"Plot actual vs predicted values for all models\"\"\"\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Plot actual values\n",
    "    ax.plot(actual_values, label='Actual', color='black', alpha=0.7)\n",
    "    \n",
    "    # Plot predictions for each model\n",
    "    for model_name, model_results in results.items():\n",
    "        predictions = model_results['predictions'][data_split]\n",
    "        ax.plot(predictions, label=model_name, alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('AQI')\n",
    "    ax.set_title(f'Actual vs Predicted AQI - {data_split.capitalize()}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_feature_importance(city, results):\n",
    "    \"\"\"Plot feature importance for models that support it\"\"\"\n",
    "    # Get models with feature importance\n",
    "    models_with_importance = {\n",
    "        name: results[name]['model_config']['feature_importance']\n",
    "        for name in results\n",
    "        if results[name]['model_config']['feature_importance'] is not None\n",
    "    }\n",
    "    \n",
    "    if not models_with_importance:\n",
    "        return None\n",
    "    \n",
    "    # Create subplots\n",
    "    n_models = len(models_with_importance)\n",
    "    fig, axes = plt.subplots(n_models, 1, figsize=(15, 5*n_models))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (model_name, importance) in enumerate(models_with_importance.items()):\n",
    "        # Sort features by importance\n",
    "        sorted_features = sorted(importance.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        features, values = zip(*sorted_features)\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = axes[idx].barh(features, values)\n",
    "        \n",
    "        # Color bars based on sign\n",
    "        for bar in bars:\n",
    "            if bar.get_width() < 0:\n",
    "                bar.set_color('red')\n",
    "            else:\n",
    "                bar.set_color('blue')\n",
    "        \n",
    "        # Add vertical line at x=0\n",
    "        axes[idx].axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        axes[idx].set_title(f'Feature Importance - {model_name.replace(\"_\", \" \").title()}')\n",
    "        axes[idx].set_xlabel('Importance')\n",
    "        axes[idx].set_ylabel('Features')\n",
    "    \n",
    "    plt.suptitle(f'Feature Importance Analysis - {city.title()}', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def save_ml_visualizations(city, results, city_data):\n",
    "    \"\"\"Save all visualizations for ML models\"\"\"\n",
    "    # Create visualization directory\n",
    "    viz_dir = f\"./notebooks/results/ml_models/{city}/visualizations\"\n",
    "    os.makedirs(viz_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate and save plots\n",
    "    fig = plot_metrics_comparison(city, results)\n",
    "    fig.savefig(f\"{viz_dir}/metrics_comparison.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Plot predictions for each split\n",
    "    for split in ['val', 'test']:\n",
    "        # Get actual values from the data\n",
    "        actual_values = city_data[split]['AQI'].values\n",
    "        fig = plot_predictions(city, results, split, actual_values)\n",
    "        fig.savefig(f\"{viz_dir}/predictions_{split}.png\", bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, let's load the preprocessed data for each city\n",
    "def load_city_data(city_name):\n",
    "    \"\"\"Load preprocessed data for a specific city\"\"\"\n",
    "    data_dir = \"../data/processed\"\n",
    "    city_data = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = f\"{data_dir}/{city_name}/{split}.csv\"\n",
    "        # print(file_path)\n",
    "        if os.path.exists(file_path):\n",
    "            city_data[split] = pd.read_csv(file_path)\n",
    "            city_data[split]['date'] = pd.to_datetime(city_data[split]['Date'])\n",
    "    \n",
    "    return city_data\n",
    "\n",
    "def run_ml_analysis():\n",
    "    \"\"\"Run ML analysis for all cities\"\"\"\n",
    "    cities = ['bengaluru', 'chennai', 'delhi', 'hyderabad']\n",
    "    \n",
    "    for city in cities:\n",
    "        print(f\"\\nProcessing {city}...\")\n",
    "        \n",
    "        # Load city data\n",
    "        city_data = load_city_data(city)\n",
    "        if not city_data:\n",
    "            print(f\"No data found for {city}\")\n",
    "            continue\n",
    "        \n",
    "        # Evaluate all ML models\n",
    "        print(\"Evaluating ML models...\")\n",
    "        ml_results = evaluate_all_ml_models(city_data, city)\n",
    "        \n",
    "        # Save results\n",
    "        print(\"Saving results...\")\n",
    "        save_ml_results(city, ml_results)\n",
    "        \n",
    "        # Generate and save visualizations\n",
    "        print(\"Generating visualizations...\")\n",
    "        save_ml_visualizations(city, ml_results, city_data)\n",
    "        \n",
    "        # Print summary metrics\n",
    "        print(\"\\nModel Performance Summary:\")\n",
    "        for model_name, results in ml_results.items():\n",
    "            print(f\"\\n{model_name.replace('_', ' ').title()}:\")\n",
    "            for split in ['val', 'test']:\n",
    "                print(f\"{split.title()} Set:\")\n",
    "                print(f\"  RMSE: {results[split][f'{split}_rmse']:.2f}\")\n",
    "                print(f\"  MAE: {results[split][f'{split}_mae']:.2f}\")\n",
    "                print(f\"  R²: {results[split][f'{split}_r2']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Update the ModelResultsManager class to handle ML results\n",
    "class ModelResultsManager:\n",
    "    def __init__(self):\n",
    "        self.base_dir = \"./results\"\n",
    "        self.ml_dir = f\"{self.base_dir}/ml_models\"\n",
    "        self._create_directory_structure()\n",
    "    \n",
    "    def _create_directory_structure(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        os.makedirs(self.ml_dir, exist_ok=True)\n",
    "        for city in ['Bengaluru', 'Chennai', 'Delhi', 'Hyderabad']:\n",
    "            city_dir = f\"{self.ml_dir}/{city}\"\n",
    "            os.makedirs(city_dir, exist_ok=True)\n",
    "            os.makedirs(f\"{city_dir}/visualizations\", exist_ok=True)\n",
    "    \n",
    "    def save_ml_results(self, city, results):\n",
    "        \"\"\"Save ML model results\"\"\"\n",
    "        city_dir = f\"{self.ml_dir}/{city}\"\n",
    "        \n",
    "        # Save results for each model\n",
    "        for model_name, model_results in results.items():\n",
    "            # Convert numpy arrays to lists for JSON serialization\n",
    "            serializable_results = {\n",
    "                'validation': model_results['validation'],\n",
    "                'test': model_results['test'],\n",
    "                'predictions': model_results['predictions'],\n",
    "                'model_config': model_results['model_config']\n",
    "            }\n",
    "            \n",
    "            # Save to file\n",
    "            with open(f\"{city_dir}/{model_name}_results.json\", 'w') as f:\n",
    "                json.dump(serializable_results, f, indent=4)\n",
    "        \n",
    "        # Update metadata\n",
    "        self._update_metadata(city, results)\n",
    "    \n",
    "    def _update_metadata(self, city, results):\n",
    "        \"\"\"Update metadata file with ML model results\"\"\"\n",
    "        metadata_file = f\"{self.base_dir}/metadata.json\"\n",
    "        \n",
    "        # Load existing metadata\n",
    "        if os.path.exists(metadata_file):\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "        else:\n",
    "            metadata = {}\n",
    "        \n",
    "        # Update ML results\n",
    "        if 'ml_models' not in metadata:\n",
    "            metadata['ml_models'] = {}\n",
    "        \n",
    "        metadata['ml_models'][city] = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'models': list(results.keys()),\n",
    "            'metrics': {\n",
    "                model: {\n",
    "                    'validation': {\n",
    "                        'rmse': results[model]['validation'][f'val_rmse'],\n",
    "                        'mae': results[model]['validation'][f'val_mae'],\n",
    "                        'r2': results[model]['validation'][f'val_r2']\n",
    "                    },\n",
    "                    'test': {\n",
    "                        'rmse': results[model]['test'][f'test_rmse'],\n",
    "                        'mae': results[model]['test'][f'test_mae'],\n",
    "                        'r2': results[model]['test'][f'test_r2']\n",
    "                    }\n",
    "                }\n",
    "                for model in results\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save updated metadata\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing bengaluru...\n",
      "Evaluating ML models...\n",
      "\n",
      "Evaluating linear_regression for bengaluru...\n",
      "\n",
      "Evaluating ridge for bengaluru...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Evaluating lasso for bengaluru...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Evaluating random_forest for bengaluru...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Evaluating gradient_boosting for bengaluru...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Evaluating xgboost for bengaluru...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Evaluating svr for bengaluru...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Saving results...\n",
      "Generating visualizations...\n",
      "\n",
      "Model Performance Summary:\n",
      "\n",
      "Linear Regression:\n",
      "Val Set:\n",
      "  RMSE: 21.87\n",
      "  MAE: 18.08\n",
      "  R²: 0.26\n",
      "Test Set:\n",
      "  RMSE: 17.28\n",
      "  MAE: 14.92\n",
      "  R²: 0.22\n",
      "\n",
      "Ridge:\n",
      "Val Set:\n",
      "  RMSE: 21.85\n",
      "  MAE: 18.08\n",
      "  R²: 0.26\n",
      "Test Set:\n",
      "  RMSE: 17.28\n",
      "  MAE: 14.93\n",
      "  R²: 0.22\n",
      "\n",
      "Lasso:\n",
      "Val Set:\n",
      "  RMSE: 20.97\n",
      "  MAE: 17.31\n",
      "  R²: 0.32\n",
      "Test Set:\n",
      "  RMSE: 16.34\n",
      "  MAE: 14.02\n",
      "  R²: 0.30\n",
      "\n",
      "Random Forest:\n",
      "Val Set:\n",
      "  RMSE: 14.66\n",
      "  MAE: 11.44\n",
      "  R²: 0.67\n",
      "Test Set:\n",
      "  RMSE: 9.32\n",
      "  MAE: 6.89\n",
      "  R²: 0.77\n",
      "\n",
      "Gradient Boosting:\n",
      "Val Set:\n",
      "  RMSE: 16.24\n",
      "  MAE: 12.73\n",
      "  R²: 0.59\n",
      "Test Set:\n",
      "  RMSE: 10.49\n",
      "  MAE: 8.21\n",
      "  R²: 0.71\n",
      "\n",
      "Xgboost:\n",
      "Val Set:\n",
      "  RMSE: 16.29\n",
      "  MAE: 12.68\n",
      "  R²: 0.59\n",
      "Test Set:\n",
      "  RMSE: 10.39\n",
      "  MAE: 8.19\n",
      "  R²: 0.72\n",
      "\n",
      "Svr:\n",
      "Val Set:\n",
      "  RMSE: 18.97\n",
      "  MAE: 15.40\n",
      "  R²: 0.45\n",
      "Test Set:\n",
      "  RMSE: 15.03\n",
      "  MAE: 12.94\n",
      "  R²: 0.41\n",
      "\n",
      "Processing chennai...\n",
      "Evaluating ML models...\n",
      "\n",
      "Evaluating linear_regression for chennai...\n",
      "\n",
      "Evaluating ridge for chennai...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Evaluating lasso for chennai...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Evaluating random_forest for chennai...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Evaluating gradient_boosting for chennai...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Evaluating xgboost for chennai...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Evaluating svr for chennai...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Saving results...\n",
      "Generating visualizations...\n",
      "\n",
      "Model Performance Summary:\n",
      "\n",
      "Linear Regression:\n",
      "Val Set:\n",
      "  RMSE: 25.63\n",
      "  MAE: 20.22\n",
      "  R²: 0.65\n",
      "Test Set:\n",
      "  RMSE: 30.17\n",
      "  MAE: 23.29\n",
      "  R²: 0.29\n",
      "\n",
      "Ridge:\n",
      "Val Set:\n",
      "  RMSE: 25.71\n",
      "  MAE: 20.01\n",
      "  R²: 0.64\n",
      "Test Set:\n",
      "  RMSE: 29.04\n",
      "  MAE: 22.21\n",
      "  R²: 0.35\n",
      "\n",
      "Lasso:\n",
      "Val Set:\n",
      "  RMSE: 25.14\n",
      "  MAE: 20.17\n",
      "  R²: 0.66\n",
      "Test Set:\n",
      "  RMSE: 25.27\n",
      "  MAE: 19.65\n",
      "  R²: 0.50\n",
      "\n",
      "Random Forest:\n",
      "Val Set:\n",
      "  RMSE: 24.15\n",
      "  MAE: 18.13\n",
      "  R²: 0.69\n",
      "Test Set:\n",
      "  RMSE: 20.24\n",
      "  MAE: 14.45\n",
      "  R²: 0.68\n",
      "\n",
      "Gradient Boosting:\n",
      "Val Set:\n",
      "  RMSE: 23.43\n",
      "  MAE: 17.80\n",
      "  R²: 0.70\n",
      "Test Set:\n",
      "  RMSE: 22.47\n",
      "  MAE: 17.52\n",
      "  R²: 0.61\n",
      "\n",
      "Xgboost:\n",
      "Val Set:\n",
      "  RMSE: 23.36\n",
      "  MAE: 17.68\n",
      "  R²: 0.71\n",
      "Test Set:\n",
      "  RMSE: 22.83\n",
      "  MAE: 17.81\n",
      "  R²: 0.60\n",
      "\n",
      "Svr:\n",
      "Val Set:\n",
      "  RMSE: 27.76\n",
      "  MAE: 21.40\n",
      "  R²: 0.59\n",
      "Test Set:\n",
      "  RMSE: 30.74\n",
      "  MAE: 22.83\n",
      "  R²: 0.27\n",
      "\n",
      "Processing delhi...\n",
      "Evaluating ML models...\n",
      "\n",
      "Evaluating linear_regression for delhi...\n",
      "\n",
      "Evaluating ridge for delhi...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Evaluating lasso for delhi...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Evaluating random_forest for delhi...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Evaluating gradient_boosting for delhi...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Evaluating xgboost for delhi...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Evaluating svr for delhi...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Saving results...\n",
      "Generating visualizations...\n",
      "\n",
      "Model Performance Summary:\n",
      "\n",
      "Linear Regression:\n",
      "Val Set:\n",
      "  RMSE: 37.30\n",
      "  MAE: 29.95\n",
      "  R²: 0.90\n",
      "Test Set:\n",
      "  RMSE: 33.19\n",
      "  MAE: 25.33\n",
      "  R²: 0.91\n",
      "\n",
      "Ridge:\n",
      "Val Set:\n",
      "  RMSE: 37.19\n",
      "  MAE: 29.86\n",
      "  R²: 0.90\n",
      "Test Set:\n",
      "  RMSE: 33.09\n",
      "  MAE: 25.24\n",
      "  R²: 0.92\n",
      "\n",
      "Lasso:\n",
      "Val Set:\n",
      "  RMSE: 32.19\n",
      "  MAE: 23.69\n",
      "  R²: 0.92\n",
      "Test Set:\n",
      "  RMSE: 33.40\n",
      "  MAE: 23.83\n",
      "  R²: 0.91\n",
      "\n",
      "Random Forest:\n",
      "Val Set:\n",
      "  RMSE: 28.84\n",
      "  MAE: 21.54\n",
      "  R²: 0.94\n",
      "Test Set:\n",
      "  RMSE: 29.84\n",
      "  MAE: 23.86\n",
      "  R²: 0.93\n",
      "\n",
      "Gradient Boosting:\n",
      "Val Set:\n",
      "  RMSE: 29.58\n",
      "  MAE: 21.40\n",
      "  R²: 0.94\n",
      "Test Set:\n",
      "  RMSE: 27.88\n",
      "  MAE: 21.27\n",
      "  R²: 0.94\n",
      "\n",
      "Xgboost:\n",
      "Val Set:\n",
      "  RMSE: 30.30\n",
      "  MAE: 21.68\n",
      "  R²: 0.93\n",
      "Test Set:\n",
      "  RMSE: 27.99\n",
      "  MAE: 20.85\n",
      "  R²: 0.94\n",
      "\n",
      "Svr:\n",
      "Val Set:\n",
      "  RMSE: 34.19\n",
      "  MAE: 26.67\n",
      "  R²: 0.91\n",
      "Test Set:\n",
      "  RMSE: 31.58\n",
      "  MAE: 23.69\n",
      "  R²: 0.92\n",
      "\n",
      "Processing hyderabad...\n",
      "Evaluating ML models...\n",
      "\n",
      "Evaluating linear_regression for hyderabad...\n",
      "\n",
      "Evaluating ridge for hyderabad...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Evaluating lasso for hyderabad...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "\n",
      "Evaluating random_forest for hyderabad...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Evaluating gradient_boosting for hyderabad...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Evaluating xgboost for hyderabad...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Evaluating svr for hyderabad...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Saving results...\n",
      "Generating visualizations...\n",
      "\n",
      "Model Performance Summary:\n",
      "\n",
      "Linear Regression:\n",
      "Val Set:\n",
      "  RMSE: 16.86\n",
      "  MAE: 14.05\n",
      "  R²: 0.60\n",
      "Test Set:\n",
      "  RMSE: 19.08\n",
      "  MAE: 15.82\n",
      "  R²: 0.71\n",
      "\n",
      "Ridge:\n",
      "Val Set:\n",
      "  RMSE: 17.09\n",
      "  MAE: 14.48\n",
      "  R²: 0.59\n",
      "Test Set:\n",
      "  RMSE: 18.98\n",
      "  MAE: 15.61\n",
      "  R²: 0.72\n",
      "\n",
      "Lasso:\n",
      "Val Set:\n",
      "  RMSE: 17.32\n",
      "  MAE: 14.80\n",
      "  R²: 0.57\n",
      "Test Set:\n",
      "  RMSE: 18.59\n",
      "  MAE: 15.04\n",
      "  R²: 0.73\n",
      "\n",
      "Random Forest:\n",
      "Val Set:\n",
      "  RMSE: 11.65\n",
      "  MAE: 9.26\n",
      "  R²: 0.81\n",
      "Test Set:\n",
      "  RMSE: 16.37\n",
      "  MAE: 13.53\n",
      "  R²: 0.79\n",
      "\n",
      "Gradient Boosting:\n",
      "Val Set:\n",
      "  RMSE: 9.96\n",
      "  MAE: 7.88\n",
      "  R²: 0.86\n",
      "Test Set:\n",
      "  RMSE: 14.01\n",
      "  MAE: 11.77\n",
      "  R²: 0.84\n",
      "\n",
      "Xgboost:\n",
      "Val Set:\n",
      "  RMSE: 9.79\n",
      "  MAE: 7.86\n",
      "  R²: 0.86\n",
      "Test Set:\n",
      "  RMSE: 14.70\n",
      "  MAE: 12.51\n",
      "  R²: 0.83\n",
      "\n",
      "Svr:\n",
      "Val Set:\n",
      "  RMSE: 14.34\n",
      "  MAE: 11.95\n",
      "  R²: 0.71\n",
      "Test Set:\n",
      "  RMSE: 16.01\n",
      "  MAE: 12.46\n",
      "  R²: 0.80\n"
     ]
    }
   ],
   "source": [
    "# Initialize results manager\n",
    "results_manager = ModelResultsManager()\n",
    "\n",
    "# Run ML analysis\n",
    "run_ml_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

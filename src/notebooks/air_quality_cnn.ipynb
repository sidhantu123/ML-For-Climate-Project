{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "\n",
    "# # Set visualization style\n",
    "# plt.style.use('seaborn')\n",
    "# sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features for bengaluru train set\n",
      "Creating lag features for bengaluru val set\n",
      "Creating lag features for bengaluru test set\n",
      "\n",
      "Bengaluru data loaded:\n",
      "Train: 1241 samples\n",
      "Validation: 287 samples\n",
      "Test: 382 samples\n",
      "Creating lag features for chennai train set\n",
      "Creating lag features for chennai val set\n",
      "Creating lag features for chennai test set\n",
      "\n",
      "Chennai data loaded:\n",
      "Train: 1224 samples\n",
      "Validation: 283 samples\n",
      "Test: 377 samples\n",
      "Creating lag features for delhi train set\n",
      "Creating lag features for delhi val set\n",
      "Creating lag features for delhi test set\n",
      "\n",
      "Delhi data loaded:\n",
      "Train: 1299 samples\n",
      "Validation: 300 samples\n",
      "Test: 400 samples\n",
      "Creating lag features for hyderabad train set\n",
      "Creating lag features for hyderabad val set\n",
      "Creating lag features for hyderabad test set\n",
      "\n",
      "Hyderabad data loaded:\n",
      "Train: 1222 samples\n",
      "Validation: 282 samples\n",
      "Test: 376 samples\n"
     ]
    }
   ],
   "source": [
    "def create_lag_features(data, target_col='AQI', n_lags=7):\n",
    "    \"\"\"Create lag features for the target column\"\"\"\n",
    "    data = data.sort_values('Date')\n",
    "    for i in range(1, n_lags + 1):\n",
    "        data[f'{target_col}_lag_{i}'] = data[target_col].shift(i)\n",
    "    return data\n",
    "\n",
    "# Cell 2: Load pre-split data for each city\n",
    "processed_dir = '../data/processed'\n",
    "cities = ['bengaluru', 'chennai', 'delhi', 'hyderabad']\n",
    "\n",
    "# Dictionary to store data for each city\n",
    "city_data = {}\n",
    "\n",
    "for city in cities:\n",
    "    city_dir = f'{processed_dir}/{city.lower()}'\n",
    "    city_data[city] = {\n",
    "        'train': pd.read_csv(f'{city_dir}/train.csv'),\n",
    "        'val': pd.read_csv(f'{city_dir}/val.csv'),\n",
    "        'test': pd.read_csv(f'{city_dir}/test.csv')\n",
    "    }\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        city_data[city][split]['Date'] = pd.to_datetime(city_data[city][split]['Date'])\n",
    "        \n",
    "        # Check if lag features exist, if not create them\n",
    "        if 'AQI_lag_1' not in city_data[city][split].columns:\n",
    "            print(f\"Creating lag features for {city} {split} set\")\n",
    "            city_data[city][split] = create_lag_features(city_data[city][split])\n",
    "    \n",
    "    print(f\"\\n{city.title()} data loaded:\")\n",
    "    print(f\"Train: {city_data[city]['train'].shape[0]} samples\")\n",
    "    print(f\"Validation: {city_data[city]['val'].shape[0]} samples\")\n",
    "    print(f\"Test: {city_data[city]['test'].shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, let's load the preprocessed data for each city\n",
    "def load_city_data(city_name):\n",
    "    \"\"\"Load preprocessed data for a specific city\"\"\"\n",
    "    data_dir = \"../data/processed\"\n",
    "    city_data = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = f\"{data_dir}/{city_name}/{split}.csv\"\n",
    "        # print(file_path)\n",
    "        if os.path.exists(file_path):\n",
    "            city_data[split] = pd.read_csv(file_path)\n",
    "            city_data[split]['date'] = pd.to_datetime(city_data[split]['Date'])\n",
    "    \n",
    "    return city_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") \n",
    "    if torch.backends.mps.is_available() \n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        \n",
    "        # Improved CNN architecture\n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Global average pooling\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cnn_data_torch(city_data):\n",
    "    numeric_cols = city_data['train'].select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != 'AQI']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(city_data['train'][numeric_cols])\n",
    "    X_val = scaler.transform(city_data['val'][numeric_cols])\n",
    "    X_test = scaler.transform(city_data['test'][numeric_cols])\n",
    "    \n",
    "    X_train = torch.tensor(X_train[:, :, np.newaxis], dtype=torch.float32).permute(0, 2, 1)\n",
    "    X_val = torch.tensor(X_val[:, :, np.newaxis], dtype=torch.float32).permute(0, 2, 1)\n",
    "    X_test = torch.tensor(X_test[:, :, np.newaxis], dtype=torch.float32).permute(0, 2, 1)\n",
    "    \n",
    "    y_train = torch.tensor(city_data['train']['AQI'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    y_val = torch.tensor(city_data['val']['AQI'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    y_test = torch.tensor(city_data['test']['AQI'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), len(numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_cnn_torch(city_data, city_name, epochs=200, batch_size=32, lr=0.001):\n",
    "    \"\"\"Train and evaluate CNN model with improved training process\"\"\"\n",
    "    print(f\"\\nüîÑ Preprocessing data for {city_name}...\")\n",
    "    \n",
    "    # Get numeric columns (excluding AQI)\n",
    "    numeric_cols = city_data['train'].select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['AQI']]\n",
    "    \n",
    "    # Handle missing values in features and target\n",
    "    X_train = city_data['train'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_train = city_data['train']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    X_val = city_data['val'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_val = city_data['val']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    X_test = city_data['test'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_test = city_data['test']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Scale targets (important for neural networks)\n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "    y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train_scaled).unsqueeze(1)\n",
    "    y_train = torch.FloatTensor(y_train_scaled)\n",
    "    X_val = torch.FloatTensor(X_val_scaled).unsqueeze(1)\n",
    "    y_val = torch.FloatTensor(y_val_scaled)\n",
    "    X_test = torch.FloatTensor(X_test_scaled).unsqueeze(1)\n",
    "    y_test = torch.FloatTensor(y_test_scaled)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"üîß Initializing model...\")\n",
    "    device = torch.device('cpu')\n",
    "    input_len = X_train.shape[2]\n",
    "    model = CNNRegressor(input_channels=1, input_length=input_len).to(device)\n",
    "    \n",
    "    # Use AdamW optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # Use MSE loss\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"üèÉ Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_true.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"‚ö†Ô∏è Early stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"üìä Evaluating model...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions\n",
    "    def get_predictions(loader):\n",
    "        preds = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                preds.extend(outputs.cpu().numpy())\n",
    "                true.extend(batch_y.numpy())\n",
    "        return np.array(preds), np.array(true)\n",
    "    \n",
    "    val_preds, val_true = get_predictions(val_loader)\n",
    "    test_preds, test_true = get_predictions(test_loader)\n",
    "    \n",
    "    # Inverse transform predictions and true values\n",
    "    val_preds = target_scaler.inverse_transform(val_preds.reshape(-1, 1)).ravel()\n",
    "    val_true = target_scaler.inverse_transform(val_true.reshape(-1, 1)).ravel()\n",
    "    test_preds = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).ravel()\n",
    "    test_true = target_scaler.inverse_transform(test_true.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def calc_metrics(y_true, y_pred):\n",
    "        return {\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'r2': r2_score(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    val_metrics = calc_metrics(val_true, val_preds)\n",
    "    test_metrics = calc_metrics(test_true, test_preds)\n",
    "    \n",
    "    print(f\"\\nüìç Evaluation Results for {city_name} (PyTorch CNN)\")\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"RMSE: {val_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {val_metrics['mae']:.2f}\")\n",
    "    print(f\"R¬≤: {val_metrics['r2']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"RMSE: {test_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {test_metrics['mae']:.2f}\")\n",
    "    print(f\"R¬≤: {test_metrics['r2']:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'val_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'predictions': {\n",
    "            'val': val_preds,\n",
    "            'test': test_preds\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Preprocessing data for delhi...\n",
      "üîß Initializing model...\n",
      "üèÉ Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2513, Val Loss: 0.0939\n",
      "Epoch [20/200], Train Loss: 0.2152, Val Loss: 0.0848\n",
      "Epoch [30/200], Train Loss: 0.1816, Val Loss: 0.0902\n",
      "‚ö†Ô∏è Early stopping at epoch 39\n",
      "üìä Evaluating model...\n",
      "\n",
      "üìç Evaluation Results for delhi (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 31.26\n",
      "MAE: 23.64\n",
      "R¬≤: 0.93\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 32.47\n",
      "MAE: 24.10\n",
      "R¬≤: 0.92\n"
     ]
    }
   ],
   "source": [
    "cnn_torch_results = train_and_evaluate_cnn_torch(city_data['delhi'], 'delhi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small CNN (our current model as baseline)\n",
    "class SmallCNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(SmallCNNRegressor, self).__init__()\n",
    "        \n",
    "        # Small architecture [32, 64, 128] -> [64, 32, 1]\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "# Medium CNN with more layers and units\n",
    "class MediumCNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(MediumCNNRegressor, self).__init__()\n",
    "        \n",
    "        # Medium architecture [64, 128, 256, 512] -> [256, 128, 64, 1]\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Large CNN with even more layers and units\n",
    "class LargeCNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(LargeCNNRegressor, self).__init__()\n",
    "        \n",
    "        # Large architecture [128, 256, 512, 1024, 2048] -> [1024, 512, 256, 128, 1]\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(1024, 2048, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "        \n",
    "\n",
    "def train_model_variant(model_class, model_name, city_data, city_name, **kwargs):\n",
    "    \"\"\"Train a specific model variant\"\"\"\n",
    "    # Default parameters\n",
    "    params = {\n",
    "        'epochs': 200,\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.001\n",
    "    }\n",
    "    params.update(kwargs)\n",
    "    \n",
    "    print(f\"\\nüîÑ Training {model_name} for {city_name}...\")\n",
    "    model = model_class(input_channels=1)\n",
    "    results = train_and_evaluate_cnn_torch(\n",
    "        city_data, \n",
    "        city_name,\n",
    "        model=model,\n",
    "        epochs=params['epochs'],\n",
    "        batch_size=params['batch_size'],\n",
    "        lr=params['learning_rate']\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "model_configs = {\n",
    "    'small_cnn': {\n",
    "        'model_class': SmallCNNRegressor,\n",
    "        'params': {\n",
    "            'batch_size': 32,\n",
    "            'epochs': 200,\n",
    "            'learning_rate': 0.001\n",
    "        }\n",
    "    },\n",
    "    'medium_cnn': {\n",
    "        'model_class': MediumCNNRegressor,\n",
    "        'params': {\n",
    "            'batch_size': 64,\n",
    "            'epochs': 200,\n",
    "            'learning_rate': 0.0005\n",
    "        }\n",
    "    },\n",
    "    'large_cnn': {\n",
    "        'model_class': LargeCNNRegressor,\n",
    "        'params': {\n",
    "            'batch_size': 128,\n",
    "            'epochs': 200,\n",
    "            'learning_rate': 0.0001\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for delhi...\n",
      "Loaded train set: 1299 samples\n",
      "Loaded val set: 300 samples\n",
      "Loaded test set: 400 samples\n"
     ]
    }
   ],
   "source": [
    "def load_city_data(city_name):\n",
    "    \"\"\"Load and prepare data for a specific city\"\"\"\n",
    "    # First, let's search for the data files\n",
    "    print(f\"Loading data for {city_name}...\")\n",
    "    \n",
    "    def create_lag_features(data, target_col='AQI', n_lags=7):\n",
    "        \"\"\"Create lag features for the target column\"\"\"\n",
    "        data = data.sort_values('Date')\n",
    "        for i in range(1, n_lags + 1):\n",
    "            data[f'{target_col}_lag_{i}'] = data[target_col].shift(i)\n",
    "        return data\n",
    "\n",
    "    # Load data from the processed directory\n",
    "    processed_dir = '../data/processed'\n",
    "    city_dir = f'{processed_dir}/{city_name.lower()}'\n",
    "    \n",
    "    city_data = {}\n",
    "    \n",
    "    # Load train, validation, and test sets\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = f'{city_dir}/{split}.csv'\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "            data['Date'] = pd.to_datetime(data['Date'])\n",
    "            \n",
    "            # Create lag features if they don't exist\n",
    "            if 'AQI_lag_1' not in data.columns:\n",
    "                data = create_lag_features(data)\n",
    "            \n",
    "            city_data[split] = data\n",
    "            print(f\"Loaded {split} set: {len(data)} samples\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find {file_path}\")\n",
    "            return None\n",
    "    \n",
    "    return city_data\n",
    "\n",
    "# Load the data before training models\n",
    "city_data = load_city_data('delhi')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Training small_cnn for delhi...\n",
      "\n",
      "üîÑ Preprocessing data for delhi...\n",
      "üîß Initializing model...\n",
      "üèÉ Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2932, Val Loss: 0.2205\n",
      "Epoch [20/200], Train Loss: 0.2125, Val Loss: 0.0690\n",
      "Epoch [30/200], Train Loss: 0.1838, Val Loss: 0.0724\n",
      "‚ö†Ô∏è Early stopping at epoch 35\n",
      "üìä Evaluating model...\n",
      "\n",
      "üìç Evaluation Results for delhi (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 31.94\n",
      "MAE: 24.72\n",
      "R¬≤: 0.93\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 33.49\n",
      "MAE: 24.74\n",
      "R¬≤: 0.91\n",
      "\n",
      "üîÑ Training medium_cnn for delhi...\n",
      "\n",
      "üîÑ Preprocessing data for delhi...\n",
      "üîß Initializing model...\n",
      "üèÉ Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2627, Val Loss: 0.1004\n",
      "Epoch [20/200], Train Loss: 0.1971, Val Loss: 0.0818\n",
      "Epoch [30/200], Train Loss: 0.1762, Val Loss: 0.0763\n",
      "Epoch [40/200], Train Loss: 0.1787, Val Loss: 0.0883\n",
      "‚ö†Ô∏è Early stopping at epoch 50\n",
      "üìä Evaluating model...\n",
      "\n",
      "üìç Evaluation Results for delhi (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 34.31\n",
      "MAE: 26.68\n",
      "R¬≤: 0.91\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 31.78\n",
      "MAE: 24.08\n",
      "R¬≤: 0.92\n",
      "\n",
      "üîÑ Training large_cnn for delhi...\n",
      "\n",
      "üîÑ Preprocessing data for delhi...\n",
      "üîß Initializing model...\n",
      "üèÉ Starting training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, config \u001b[38;5;129;01min\u001b[39;00m model_configs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 4\u001b[0m     results[model_name] \u001b[38;5;241m=\u001b[39m train_model_variant(\n\u001b[1;32m      5\u001b[0m         config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_class\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m         model_name,\n\u001b[1;32m      7\u001b[0m         city_data,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelhi\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[38], line 161\u001b[0m, in \u001b[0;36mtrain_model_variant\u001b[0;34m(model_class, model_name, city_data, city_name, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîÑ Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m results \u001b[38;5;241m=\u001b[39m train_and_evaluate_cnn_torch(\n\u001b[1;32m    162\u001b[0m     city_data, \n\u001b[1;32m    163\u001b[0m     city_name,\n\u001b[1;32m    164\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    165\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    166\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    167\u001b[0m     lr\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    168\u001b[0m )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[27], line 75\u001b[0m, in \u001b[0;36mtrain_and_evaluate_cnn_torch\u001b[0;34m(city_data, city_name, epochs, batch_size, lr, model)\u001b[0m\n\u001b[1;32m     72\u001b[0m batch_X, batch_y \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     74\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 75\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_X)\n\u001b[1;32m     76\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[1;32m     77\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[38], line 142\u001b[0m, in \u001b[0;36mLargeCNNRegressor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[1;32m    143\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    372\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train all variants\n",
    "results = {}\n",
    "for model_name, config in model_configs.items():\n",
    "    results[model_name] = train_model_variant(\n",
    "        config['model_class'],\n",
    "        model_name,\n",
    "        city_data,\n",
    "        'delhi',\n",
    "        **config['params']\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

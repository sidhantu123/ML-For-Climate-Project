{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "\n",
    "# # Set visualization style\n",
    "# plt.style.use('seaborn')\n",
    "# sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features for bengaluru train set\n",
      "Creating lag features for bengaluru val set\n",
      "Creating lag features for bengaluru test set\n",
      "\n",
      "Bengaluru data loaded:\n",
      "Train: 1241 samples\n",
      "Validation: 287 samples\n",
      "Test: 382 samples\n",
      "Creating lag features for chennai train set\n",
      "Creating lag features for chennai val set\n",
      "Creating lag features for chennai test set\n",
      "\n",
      "Chennai data loaded:\n",
      "Train: 1224 samples\n",
      "Validation: 283 samples\n",
      "Test: 377 samples\n",
      "Creating lag features for delhi train set\n",
      "Creating lag features for delhi val set\n",
      "Creating lag features for delhi test set\n",
      "\n",
      "Delhi data loaded:\n",
      "Train: 1299 samples\n",
      "Validation: 300 samples\n",
      "Test: 400 samples\n",
      "Creating lag features for hyderabad train set\n",
      "Creating lag features for hyderabad val set\n",
      "Creating lag features for hyderabad test set\n",
      "\n",
      "Hyderabad data loaded:\n",
      "Train: 1222 samples\n",
      "Validation: 282 samples\n",
      "Test: 376 samples\n"
     ]
    }
   ],
   "source": [
    "def create_lag_features(data, target_col='AQI', n_lags=7):\n",
    "    \"\"\"Create lag features for the target column\"\"\"\n",
    "    data = data.sort_values('Date')\n",
    "    for i in range(1, n_lags + 1):\n",
    "        data[f'{target_col}_lag_{i}'] = data[target_col].shift(i)\n",
    "    return data\n",
    "\n",
    "# Cell 2: Load pre-split data for each city\n",
    "processed_dir = '../data/processed'\n",
    "cities = ['bengaluru', 'chennai', 'delhi', 'hyderabad']\n",
    "\n",
    "# Dictionary to store data for each city\n",
    "city_data = {}\n",
    "\n",
    "for city in cities:\n",
    "    city_dir = f'{processed_dir}/{city.lower()}'\n",
    "    city_data[city] = {\n",
    "        'train': pd.read_csv(f'{city_dir}/train.csv'),\n",
    "        'val': pd.read_csv(f'{city_dir}/val.csv'),\n",
    "        'test': pd.read_csv(f'{city_dir}/test.csv')\n",
    "    }\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        city_data[city][split]['Date'] = pd.to_datetime(city_data[city][split]['Date'])\n",
    "        \n",
    "        # Check if lag features exist, if not create them\n",
    "        if 'AQI_lag_1' not in city_data[city][split].columns:\n",
    "            print(f\"Creating lag features for {city} {split} set\")\n",
    "            city_data[city][split] = create_lag_features(city_data[city][split])\n",
    "    \n",
    "    print(f\"\\n{city.title()} data loaded:\")\n",
    "    print(f\"Train: {city_data[city]['train'].shape[0]} samples\")\n",
    "    print(f\"Validation: {city_data[city]['val'].shape[0]} samples\")\n",
    "    print(f\"Test: {city_data[city]['test'].shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, let's load the preprocessed data for each city\n",
    "def load_city_data(city_name):\n",
    "    \"\"\"Load preprocessed data for a specific city\"\"\"\n",
    "    data_dir = \"../data/processed\"\n",
    "    city_data = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = f\"{data_dir}/{city_name}/{split}.csv\"\n",
    "        # print(file_path)\n",
    "        if os.path.exists(file_path):\n",
    "            city_data[split] = pd.read_csv(file_path)\n",
    "            city_data[split]['date'] = pd.to_datetime(city_data[split]['Date'])\n",
    "    \n",
    "    return city_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") \n",
    "    if torch.backends.mps.is_available() \n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        \n",
    "        # Improved CNN architecture\n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Global average pooling\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cnn_feature_importance(model, X_val, feature_names):\n",
    "    \"\"\"Calculate feature importance for CNN model using gradient-based method\n",
    "    \n",
    "    Args:\n",
    "        model (CNNRegressor): Trained CNN model\n",
    "        X_val (torch.Tensor): Validation input data\n",
    "        feature_names (list): List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping feature names to their importance scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    X_val = X_val.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(X_val)\n",
    "    \n",
    "    # Calculate gradients with respect to input\n",
    "    output.backward(torch.ones_like(output))\n",
    "    \n",
    "    # Get absolute gradients and average across samples\n",
    "    gradients = torch.abs(X_val.grad)\n",
    "    importance = torch.mean(gradients, dim=0).squeeze()\n",
    "    \n",
    "    # Convert to dictionary format and move to CPU if needed\n",
    "    feature_importance = dict(zip(feature_names, importance.detach().cpu().numpy()))\n",
    "    \n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(city, results):\n",
    "    \"\"\"Plot comparison of metrics for different models\n",
    "    \n",
    "    Args:\n",
    "        city (str): Name of the city\n",
    "        results (dict): Dictionary containing model metrics\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: Figure object containing the plots\n",
    "    \"\"\"\n",
    "    # Define metrics and splits\n",
    "    metrics = ['rmse', 'mae', 'r2']\n",
    "    splits = ['val', 'test']\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(6*len(metrics), 6))\n",
    "    if len(metrics) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Collect values for each split\n",
    "        values = {split: [] for split in splits}\n",
    "        \n",
    "        for split in splits:\n",
    "            metric_key = f'{split}_{metric}'\n",
    "            if metric_key in results:\n",
    "                values[split].append(results[metric_key])\n",
    "        \n",
    "        # Plot bars\n",
    "        x = np.arange(1)\n",
    "        width = 0.35\n",
    "        \n",
    "        for j, split in enumerate(splits):\n",
    "            if values[split]:\n",
    "                ax.bar(x + j*width, values[split], width, label=split.capitalize())\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_ylabel(metric.upper())\n",
    "        ax.set_title(f'{metric.upper()} Comparison')\n",
    "        ax.set_xticks(x + width/2)\n",
    "        ax.set_xticklabels(['CNN'])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Model Performance Comparison - {city.title()}', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(city, feature_importance):\n",
    "    \"\"\"Plot feature importance for CNN model\n",
    "    \n",
    "    Args:\n",
    "        city (str): Name of the city\n",
    "        feature_importance (dict): Dictionary mapping features to their importance scores\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: Figure object containing the plot\n",
    "    \"\"\"\n",
    "    if not feature_importance:\n",
    "        print(\"No feature importance data available\")\n",
    "        return\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_features = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    features, values = zip(*sorted_features)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.barh(features, values)\n",
    "    \n",
    "    # Color bars based on sign\n",
    "    for bar in bars:\n",
    "        if bar.get_width() < 0:\n",
    "            bar.set_color('red')\n",
    "        else:\n",
    "            bar.set_color('blue')\n",
    "    \n",
    "    # Add vertical line at x=0\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_title(f'Feature Importance - CNN ({city.title()})')\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_ylabel('Features')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(city, results, data_split, actual_values):\n",
    "    \"\"\"Plot actual vs predicted values\n",
    "    \n",
    "    Args:\n",
    "        city (str): Name of the city\n",
    "        results (dict): Dictionary containing model predictions\n",
    "        data_split (str): Either 'val' or 'test'\n",
    "        actual_values (array-like): Actual target values\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: Figure object containing the plot\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Plot actual values\n",
    "    ax.plot(actual_values, label='Actual', color='black', alpha=0.7)\n",
    "    \n",
    "    # Plot predictions\n",
    "    predictions = results['predictions'][data_split]\n",
    "    ax.plot(predictions, label='CNN', alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('AQI')\n",
    "    ax.set_title(f'Actual vs Predicted AQI - {data_split.capitalize()}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cnn_results(city, results, feature_importance, model):\n",
    "    \"\"\"Save CNN model results and visualizations\"\"\"\n",
    "    # Create results directory with a simple model name\n",
    "    model_name = model.__class__.__name__  # This will give us just 'SmallCNNRegressor' or similar\n",
    "    results_dir = f\"./results/cnn_models/{model_name}/{city}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{results_dir}/visualizations\", exist_ok=True)\n",
    "    \n",
    "    # Convert numpy arrays and float32 to native Python types\n",
    "    metrics = {\n",
    "        'val_rmse': float(results['val_metrics']['rmse']),\n",
    "        'val_mae': float(results['val_metrics']['mae']),\n",
    "        'val_r2': float(results['val_metrics']['r2']),\n",
    "        'test_rmse': float(results['test_metrics']['rmse']),\n",
    "        'test_mae': float(results['test_metrics']['mae']),\n",
    "        'test_r2': float(results['test_metrics']['r2']),\n",
    "        'model_config': {\n",
    "            'name': model_name,\n",
    "            'parameters': {\n",
    "                'epochs': 200,\n",
    "                'batch_size': 32,\n",
    "                'learning_rate': 0.001\n",
    "            },\n",
    "            'feature_importance': {k: float(v) for k, v in feature_importance.items()}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save metrics to JSON file\n",
    "    with open(f\"{results_dir}/cnn_results.json\", 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Generate and save plots\n",
    "    fig = plot_metrics_comparison(city, metrics)\n",
    "    fig.savefig(f\"{results_dir}/visualizations/metrics_comparison.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = plot_feature_importance(city, feature_importance)\n",
    "    fig.savefig(f\"{results_dir}/visualizations/feature_importance.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Plot predictions for each split\n",
    "    for split in ['val', 'test']:\n",
    "        actual_values = results['actual_values'][split]\n",
    "        fig = plot_predictions(city, results, split, actual_values)\n",
    "        fig.savefig(f\"{results_dir}/visualizations/predictions_{split}.png\", bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cnn_data_torch(city_data):\n",
    "    numeric_cols = city_data['train'].select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != 'AQI']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(city_data['train'][numeric_cols])\n",
    "    X_val = scaler.transform(city_data['val'][numeric_cols])\n",
    "    X_test = scaler.transform(city_data['test'][numeric_cols])\n",
    "    \n",
    "    X_train = torch.tensor(X_train[:, :, np.newaxis], dtype=torch.float32).permute(0, 2, 1)\n",
    "    X_val = torch.tensor(X_val[:, :, np.newaxis], dtype=torch.float32).permute(0, 2, 1)\n",
    "    X_test = torch.tensor(X_test[:, :, np.newaxis], dtype=torch.float32).permute(0, 2, 1)\n",
    "    \n",
    "    y_train = torch.tensor(city_data['train']['AQI'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    y_val = torch.tensor(city_data['val']['AQI'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    y_test = torch.tensor(city_data['test']['AQI'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), len(numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_cnn_torch(city_data, city_name, epochs=200, batch_size=32, lr=0.001):\n",
    "    \"\"\"Train and evaluate CNN model with improved training process\"\"\"\n",
    "    print(f\"\\n🔄 Preprocessing data for {city_name}...\")\n",
    "    \n",
    "    # Get numeric columns (excluding AQI)\n",
    "    numeric_cols = city_data['train'].select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['AQI']]\n",
    "    \n",
    "    # Handle missing values in features and target\n",
    "    X_train = city_data['train'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_train = city_data['train']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    X_val = city_data['val'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_val = city_data['val']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    X_test = city_data['test'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_test = city_data['test']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Scale targets (important for neural networks)\n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "    y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train_scaled).unsqueeze(1)\n",
    "    y_train = torch.FloatTensor(y_train_scaled)\n",
    "    X_val = torch.FloatTensor(X_val_scaled).unsqueeze(1)\n",
    "    y_val = torch.FloatTensor(y_val_scaled)\n",
    "    X_test = torch.FloatTensor(X_test_scaled).unsqueeze(1)\n",
    "    y_test = torch.FloatTensor(y_test_scaled)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"🔧 Initializing model...\")\n",
    "    device = torch.device('cpu')\n",
    "    input_len = X_train.shape[2]\n",
    "    model = CNNRegressor(input_channels=1, input_length=input_len).to(device)\n",
    "    \n",
    "    # Use AdamW optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # Use MSE loss\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"🏃 Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_true.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"⚠️ Early stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"📊 Evaluating model...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions\n",
    "    def get_predictions(loader):\n",
    "        preds = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                preds.extend(outputs.cpu().numpy())\n",
    "                true.extend(batch_y.numpy())\n",
    "        return np.array(preds), np.array(true)\n",
    "    \n",
    "    val_preds, val_true = get_predictions(val_loader)\n",
    "    test_preds, test_true = get_predictions(test_loader)\n",
    "    \n",
    "    # Inverse transform predictions and true values\n",
    "    val_preds = target_scaler.inverse_transform(val_preds.reshape(-1, 1)).ravel()\n",
    "    val_true = target_scaler.inverse_transform(val_true.reshape(-1, 1)).ravel()\n",
    "    test_preds = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).ravel()\n",
    "    test_true = target_scaler.inverse_transform(test_true.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def calc_metrics(y_true, y_pred):\n",
    "        return {\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'r2': r2_score(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    val_metrics = calc_metrics(val_true, val_preds)\n",
    "    test_metrics = calc_metrics(test_true, test_preds)\n",
    "    \n",
    "    print(f\"\\n📍 Evaluation Results for {city_name} (PyTorch CNN)\")\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"RMSE: {val_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {val_metrics['mae']:.2f}\")\n",
    "    print(f\"R²: {val_metrics['r2']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"RMSE: {test_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {test_metrics['mae']:.2f}\")\n",
    "    print(f\"R²: {test_metrics['r2']:.2f}\")\n",
    "    # Calculate feature importance\n",
    "    feature_importance = calculate_cnn_feature_importance(model, X_val, numeric_cols)\n",
    "\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'val_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'predictions': {\n",
    "            'val': val_preds,\n",
    "            'test': test_preds\n",
    "        },\n",
    "        'actual_values': {\n",
    "            'val': y_val.numpy(),\n",
    "            'test': y_test.numpy()\n",
    "        }\n",
    "    }\n",
    "    save_cnn_results(city_name, results, feature_importance)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Preprocessing data for delhi...\n",
      "🔧 Initializing model...\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2529, Val Loss: 0.0784\n",
      "Epoch [20/200], Train Loss: 0.1955, Val Loss: 0.0695\n",
      "Epoch [30/200], Train Loss: 0.1803, Val Loss: 0.0846\n",
      "⚠️ Early stopping at epoch 33\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for delhi (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 32.59\n",
      "MAE: 25.21\n",
      "R²: 0.92\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 34.40\n",
      "MAE: 25.40\n",
      "R²: 0.91\n"
     ]
    }
   ],
   "source": [
    "# cnn_torch_results = train_and_evaluate_cnn_torch(city_data['delhi'], 'delhi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_cnn_torch_v2(city_data, city_name, model, epochs=200, batch_size=32, lr=0.001):\n",
    "    \"\"\"Train and evaluate CNN model with improved training process - Version 2 with model parameter support\"\"\"\n",
    "    print(f\"\\n🔄 Preprocessing data for {city_name}...\")\n",
    "    \n",
    "    # Get numeric columns (excluding AQI)\n",
    "    numeric_cols = city_data['train'].select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['AQI']]\n",
    "    \n",
    "    # Handle missing values in features and target\n",
    "    X_train = city_data['train'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_train = city_data['train']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    X_val = city_data['val'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_val = city_data['val']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    X_test = city_data['test'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_test = city_data['test']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Scale targets (important for neural networks)\n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "    y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train_scaled).unsqueeze(1)\n",
    "    y_train = torch.FloatTensor(y_train_scaled)\n",
    "    X_val = torch.FloatTensor(X_val_scaled).unsqueeze(1)\n",
    "    y_val = torch.FloatTensor(y_val_scaled)\n",
    "    X_test = torch.FloatTensor(X_test_scaled).unsqueeze(1)\n",
    "    y_test = torch.FloatTensor(y_test_scaled)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"🔧 Initializing model...\")\n",
    "    \n",
    "    print(f\"Using model: {model.__class__.__name__}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Use AdamW optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # Use MSE loss\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"🏃 Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_true.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"⚠️ Early stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"📊 Evaluating model...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions\n",
    "    def get_predictions(loader):\n",
    "        preds = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                preds.extend(outputs.cpu().numpy())\n",
    "                true.extend(batch_y.numpy())\n",
    "        return np.array(preds), np.array(true)\n",
    "    \n",
    "    val_preds, val_true = get_predictions(val_loader)\n",
    "    test_preds, test_true = get_predictions(test_loader)\n",
    "    \n",
    "    # Inverse transform predictions and true values\n",
    "    val_preds = target_scaler.inverse_transform(val_preds.reshape(-1, 1)).ravel()\n",
    "    val_true = target_scaler.inverse_transform(val_true.reshape(-1, 1)).ravel()\n",
    "    test_preds = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).ravel()\n",
    "    test_true = target_scaler.inverse_transform(test_true.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def calc_metrics(y_true, y_pred):\n",
    "        return {\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'r2': r2_score(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    val_metrics = calc_metrics(val_true, val_preds)\n",
    "    test_metrics = calc_metrics(test_true, test_preds)\n",
    "    \n",
    "    print(f\"\\n📍 Evaluation Results for {city_name} (PyTorch CNN)\")\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"RMSE: {val_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {val_metrics['mae']:.2f}\")\n",
    "    print(f\"R²: {val_metrics['r2']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"RMSE: {test_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {test_metrics['mae']:.2f}\")\n",
    "    print(f\"R²: {test_metrics['r2']:.2f}\")\n",
    "    \n",
    "     # Calculate feature importance\n",
    "    feature_importance = calculate_cnn_feature_importance(model, X_val, numeric_cols)\n",
    "\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'val_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'predictions': {\n",
    "            'val': val_preds,\n",
    "            'test': test_preds\n",
    "        },\n",
    "        'actual_values': {\n",
    "            'val': y_val.numpy(),\n",
    "            'test': y_test.numpy()\n",
    "        }\n",
    "    }\n",
    "    save_cnn_results(city_name, results, feature_importance, model)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small CNN (our current model as baseline)\n",
    "class SmallCNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(SmallCNNRegressor, self).__init__()\n",
    "        \n",
    "        # Small architecture [32, 64, 128] -> [64, 32, 1]\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "# Medium CNN with more layers and units\n",
    "class MediumCNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(MediumCNNRegressor, self).__init__()\n",
    "        \n",
    "        # Medium architecture [64, 128, 256, 512] -> [256, 128, 64, 1]\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Large CNN with even more layers and units\n",
    "class LargeCNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(LargeCNNRegressor, self).__init__()\n",
    "        \n",
    "        # Large architecture [128, 256, 512, 1024, 2048] -> [1024, 512, 256, 128, 1]\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(1024, 2048, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "        \n",
    "\n",
    "def train_model_variant(model_class, model_name, city_data, city_name, **kwargs):\n",
    "    \"\"\"Train a specific model variant\"\"\"\n",
    "    # Default parameters\n",
    "    params = {\n",
    "        'epochs': 200,\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.001\n",
    "    }\n",
    "    params.update(kwargs)\n",
    "    \n",
    "    print(f\"\\n🔄 Training {model_name} for {city_name}...\")\n",
    "    model = model_class(input_channels=1)\n",
    "    results = train_and_evaluate_cnn_torch_v2(\n",
    "        city_data, \n",
    "        city_name,\n",
    "        model=model,\n",
    "        epochs=params['epochs'],\n",
    "        batch_size=params['batch_size'],\n",
    "        lr=params['learning_rate']\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "model_configs = {\n",
    "    'small_cnn': {\n",
    "        'model_class': SmallCNNRegressor,\n",
    "        'params': {\n",
    "            'batch_size': 32,\n",
    "            'epochs': 200,\n",
    "            'learning_rate': 0.001\n",
    "        }\n",
    "    },\n",
    "    'medium_cnn': {\n",
    "        'model_class': MediumCNNRegressor,\n",
    "        'params': {\n",
    "            'batch_size': 64,\n",
    "            'epochs': 200,\n",
    "            'learning_rate': 0.0005\n",
    "        }\n",
    "    },\n",
    "    'large_cnn': {\n",
    "        'model_class': LargeCNNRegressor,\n",
    "        'params': {\n",
    "            'batch_size': 32,\n",
    "            'epochs': 200,\n",
    "            'learning_rate': 0.0005\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_city_data(city_name):\n",
    "    \"\"\"Load and prepare data for a specific city\"\"\"\n",
    "    # First, let's search for the data files\n",
    "    print(f\"Loading data for {city_name}...\")\n",
    "    \n",
    "    def create_lag_features(data, target_col='AQI', n_lags=7):\n",
    "        \"\"\"Create lag features for the target column\"\"\"\n",
    "        data = data.sort_values('Date')\n",
    "        for i in range(1, n_lags + 1):\n",
    "            data[f'{target_col}_lag_{i}'] = data[target_col].shift(i)\n",
    "        return data\n",
    "\n",
    "    # Load data from the processed directory\n",
    "    processed_dir = '../data/processed'\n",
    "    city_dir = f'{processed_dir}/{city_name.lower()}'\n",
    "    \n",
    "    city_data = {}\n",
    "    \n",
    "    # Load train, validation, and test sets\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = f'{city_dir}/{split}.csv'\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "            data['Date'] = pd.to_datetime(data['Date'])\n",
    "            \n",
    "            # Create lag features if they don't exist\n",
    "            if 'AQI_lag_1' not in data.columns:\n",
    "                data = create_lag_features(data)\n",
    "            \n",
    "            city_data[split] = data\n",
    "            print(f\"Loaded {split} set: {len(data)} samples\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find {file_path}\")\n",
    "            return None\n",
    "    \n",
    "    return city_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for delhi...\n",
      "Loaded train set: 1299 samples\n",
      "Loaded val set: 300 samples\n",
      "Loaded test set: 400 samples\n",
      "\n",
      "🔄 Training small_cnn for delhi...\n",
      "\n",
      "🔄 Preprocessing data for delhi...\n",
      "🔧 Initializing model...\n",
      "Using model: SmallCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.3076, Val Loss: 0.2369\n",
      "Epoch [20/200], Train Loss: 0.2104, Val Loss: 0.1015\n",
      "Epoch [30/200], Train Loss: 0.1927, Val Loss: 0.0861\n",
      "Epoch [40/200], Train Loss: 0.1758, Val Loss: 0.0900\n",
      "⚠️ Early stopping at epoch 41\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for delhi (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 34.18\n",
      "MAE: 26.55\n",
      "R²: 0.91\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 33.81\n",
      "MAE: 24.87\n",
      "R²: 0.91\n",
      "\n",
      "🔄 Training medium_cnn for delhi...\n",
      "\n",
      "🔄 Preprocessing data for delhi...\n",
      "🔧 Initializing model...\n",
      "Using model: MediumCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2203, Val Loss: 0.1189\n",
      "Epoch [20/200], Train Loss: 0.2003, Val Loss: 0.0720\n",
      "Epoch [30/200], Train Loss: 0.1783, Val Loss: 0.0758\n",
      "Epoch [40/200], Train Loss: 0.1674, Val Loss: 0.0794\n",
      "⚠️ Early stopping at epoch 46\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for delhi (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 31.14\n",
      "MAE: 24.16\n",
      "R²: 0.93\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 32.26\n",
      "MAE: 23.84\n",
      "R²: 0.92\n",
      "\n",
      "🔄 Training large_cnn for delhi...\n",
      "\n",
      "🔄 Preprocessing data for delhi...\n",
      "🔧 Initializing model...\n",
      "Using model: LargeCNNRegressor\n",
      "🏃 Starting training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m city_data \u001b[38;5;241m=\u001b[39m load_city_data(city)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, config \u001b[38;5;129;01min\u001b[39;00m model_configs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 8\u001b[0m     results[model_name] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_variant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcity_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 161\u001b[0m, in \u001b[0;36mtrain_model_variant\u001b[0;34m(model_class, model_name, city_data, city_name, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🔄 Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_cnn_torch_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcity_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcity_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[43], line 75\u001b[0m, in \u001b[0;36mtrain_and_evaluate_cnn_torch_v2\u001b[0;34m(city_data, city_name, model, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     73\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_X)\n\u001b[1;32m     74\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m---> 75\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m     78\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/Columbia/Spring 2025/Machine Learning and Climate/ML-For-Climate-Project/venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Columbia/Spring 2025/Machine Learning and Climate/ML-For-Climate-Project/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Columbia/Spring 2025/Machine Learning and Climate/ML-For-Climate-Project/venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train all variants\n",
    "results = {}\n",
    "cities = [\"delhi\", \"bengaluru\", \"chennai\", \"hyderabad\"]\n",
    "\n",
    "for city in cities:\n",
    "    city_data = load_city_data(city)\n",
    "    for model_name, config in model_configs.items():\n",
    "        results[model_name] = train_model_variant(\n",
    "            config['model_class'],\n",
    "            model_name,\n",
    "            city_data,\n",
    "            city,\n",
    "            **config['params']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Training small_cnn for bengaluru...\n",
      "\n",
      "🔄 Preprocessing data for bengaluru...\n",
      "🔧 Initializing model...\n",
      "Using model: SmallCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.5075, Val Loss: 0.1078\n",
      "Epoch [20/200], Train Loss: 0.4118, Val Loss: 0.0739\n",
      "Epoch [30/200], Train Loss: 0.3857, Val Loss: 0.0640\n",
      "Epoch [40/200], Train Loss: 0.3019, Val Loss: 0.0638\n",
      "Epoch [50/200], Train Loss: 0.3236, Val Loss: 0.0609\n",
      "Epoch [60/200], Train Loss: 0.2905, Val Loss: 0.0674\n",
      "Epoch [70/200], Train Loss: 0.3362, Val Loss: 0.0637\n",
      "Epoch [80/200], Train Loss: 0.3008, Val Loss: 0.0727\n",
      "⚠️ Early stopping at epoch 90\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for bengaluru (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 14.86\n",
      "MAE: 11.23\n",
      "R²: 0.69\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 16.75\n",
      "MAE: 13.16\n",
      "R²: 0.78\n",
      "\n",
      "🔄 Training medium_cnn for bengaluru...\n",
      "\n",
      "🔄 Preprocessing data for bengaluru...\n",
      "🔧 Initializing model...\n",
      "Using model: MediumCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.4627, Val Loss: 0.0714\n",
      "Epoch [20/200], Train Loss: 0.3843, Val Loss: 0.0611\n",
      "Epoch [30/200], Train Loss: 0.3360, Val Loss: 0.0690\n",
      "Epoch [40/200], Train Loss: 0.3331, Val Loss: 0.0536\n",
      "Epoch [50/200], Train Loss: 0.2988, Val Loss: 0.0528\n",
      "⚠️ Early stopping at epoch 51\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for bengaluru (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 13.72\n",
      "MAE: 10.66\n",
      "R²: 0.73\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 15.81\n",
      "MAE: 12.62\n",
      "R²: 0.80\n",
      "\n",
      "🔄 Training large_cnn for bengaluru...\n",
      "\n",
      "🔄 Preprocessing data for bengaluru...\n",
      "🔧 Initializing model...\n",
      "Using model: LargeCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.4669, Val Loss: 0.2903\n",
      "Epoch [20/200], Train Loss: 0.4114, Val Loss: 0.0561\n",
      "Epoch [30/200], Train Loss: 0.3152, Val Loss: 0.0442\n",
      "Epoch [40/200], Train Loss: 0.4085, Val Loss: 0.0692\n",
      "⚠️ Early stopping at epoch 45\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for bengaluru (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 14.49\n",
      "MAE: 11.57\n",
      "R²: 0.70\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 17.24\n",
      "MAE: 14.27\n",
      "R²: 0.77\n"
     ]
    }
   ],
   "source": [
    "# # Train all variants\n",
    "# results = {}\n",
    "# for model_name, config in model_configs.items():\n",
    "#     results[model_name] = train_model_variant(\n",
    "#         config['model_class'],\n",
    "#         model_name,\n",
    "#         city_data,\n",
    "#         'bengaluru',\n",
    "#         **config['params']\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Training small_cnn for chennai...\n",
      "\n",
      "🔄 Preprocessing data for chennai...\n",
      "🔧 Initializing model...\n",
      "Using model: SmallCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.5495, Val Loss: 0.1582\n",
      "Epoch [20/200], Train Loss: 0.4220, Val Loss: 0.0642\n",
      "Epoch [30/200], Train Loss: 0.2935, Val Loss: 0.0779\n",
      "⚠️ Early stopping at epoch 40\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for chennai (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 14.92\n",
      "MAE: 11.57\n",
      "R²: 0.68\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 16.73\n",
      "MAE: 13.11\n",
      "R²: 0.78\n",
      "\n",
      "🔄 Training medium_cnn for chennai...\n",
      "\n",
      "🔄 Preprocessing data for chennai...\n",
      "🔧 Initializing model...\n",
      "Using model: MediumCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.4508, Val Loss: 0.0827\n",
      "Epoch [20/200], Train Loss: 0.3624, Val Loss: 0.0568\n",
      "Epoch [30/200], Train Loss: 0.3611, Val Loss: 0.0608\n",
      "Epoch [40/200], Train Loss: 0.3165, Val Loss: 0.0466\n",
      "⚠️ Early stopping at epoch 42\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for chennai (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 12.87\n",
      "MAE: 9.82\n",
      "R²: 0.77\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 15.16\n",
      "MAE: 11.96\n",
      "R²: 0.82\n",
      "\n",
      "🔄 Training large_cnn for chennai...\n",
      "\n",
      "🔄 Preprocessing data for chennai...\n",
      "🔧 Initializing model...\n",
      "Using model: LargeCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.5346, Val Loss: 0.2152\n",
      "Epoch [20/200], Train Loss: 0.3985, Val Loss: 0.0790\n",
      "Epoch [30/200], Train Loss: 0.2765, Val Loss: 0.0813\n",
      "Epoch [40/200], Train Loss: 0.2367, Val Loss: 0.0628\n",
      "⚠️ Early stopping at epoch 42\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for chennai (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 14.18\n",
      "MAE: 11.12\n",
      "R²: 0.71\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 15.17\n",
      "MAE: 12.35\n",
      "R²: 0.82\n"
     ]
    }
   ],
   "source": [
    "# # Train all variants\n",
    "# results = {}\n",
    "# for model_name, config in model_configs.items():\n",
    "#     results[model_name] = train_model_variant(\n",
    "#         config['model_class'],\n",
    "#         model_name,\n",
    "#         city_data,\n",
    "#         'chennai',\n",
    "#         **config['params']\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Training small_cnn for hyderabad...\n",
      "\n",
      "🔄 Preprocessing data for hyderabad...\n",
      "🔧 Initializing model...\n",
      "Using model: SmallCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2407, Val Loss: 0.1460\n",
      "Epoch [20/200], Train Loss: 0.1944, Val Loss: 0.0876\n",
      "⚠️ Early stopping at epoch 26\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for hyderabad (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 38.95\n",
      "MAE: 30.86\n",
      "R²: 0.89\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 36.83\n",
      "MAE: 27.83\n",
      "R²: 0.90\n",
      "\n",
      "🔄 Training medium_cnn for hyderabad...\n",
      "\n",
      "🔄 Preprocessing data for hyderabad...\n",
      "🔧 Initializing model...\n",
      "Using model: MediumCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2466, Val Loss: 0.1416\n",
      "Epoch [20/200], Train Loss: 0.1919, Val Loss: 0.0654\n",
      "Epoch [30/200], Train Loss: 0.1764, Val Loss: 0.1036\n",
      "⚠️ Early stopping at epoch 35\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for hyderabad (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 32.71\n",
      "MAE: 26.08\n",
      "R²: 0.92\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 33.90\n",
      "MAE: 25.55\n",
      "R²: 0.91\n",
      "\n",
      "🔄 Training large_cnn for hyderabad...\n",
      "\n",
      "🔄 Preprocessing data for hyderabad...\n",
      "🔧 Initializing model...\n",
      "Using model: LargeCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2329, Val Loss: 0.1139\n",
      "Epoch [20/200], Train Loss: 0.1880, Val Loss: 0.0884\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, config \u001b[38;5;129;01min\u001b[39;00m model_configs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 4\u001b[0m     results[model_name] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_variant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcity_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyderabad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 161\u001b[0m, in \u001b[0;36mtrain_model_variant\u001b[0;34m(model_class, model_name, city_data, city_name, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🔄 Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_cnn_torch_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcity_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcity_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[43], line 80\u001b[0m, in \u001b[0;36mtrain_and_evaluate_cnn_torch_v2\u001b[0;34m(city_data, city_name, model, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "File \u001b[0;32m~/Columbia/Spring 2025/Machine Learning and Climate/ML-For-Climate-Project/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Columbia/Spring 2025/Machine Learning and Climate/ML-For-Climate-Project/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Columbia/Spring 2025/Machine Learning and Climate/ML-For-Climate-Project/venv/lib/python3.10/site-packages/torch/optim/adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    237\u001b[0m         group,\n\u001b[1;32m    238\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         state_steps,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Columbia/Spring 2025/Machine Learning and Climate/ML-For-Climate-Project/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Columbia/Spring 2025/Machine Learning and Climate/ML-For-Climate-Project/venv/lib/python3.10/site-packages/torch/optim/adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Columbia/Spring 2025/Machine Learning and Climate/ML-For-Climate-Project/venv/lib/python3.10/site-packages/torch/optim/adam.py:525\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    523\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 525\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Train all variants\n",
    "# results = {}\n",
    "# for model_name, config in model_configs.items():\n",
    "#     results[model_name] = train_model_variant(\n",
    "#         config['model_class'],\n",
    "#         model_name,\n",
    "#         city_data,\n",
    "#         'hyderabad',\n",
    "#         **config['params']\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

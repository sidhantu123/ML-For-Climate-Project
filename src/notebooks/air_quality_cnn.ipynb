{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "\n",
    "# # Set visualization style\n",
    "# plt.style.use('seaborn')\n",
    "# sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features for bengaluru train set\n",
      "Creating lag features for bengaluru val set\n",
      "Creating lag features for bengaluru test set\n",
      "\n",
      "Bengaluru data loaded:\n",
      "Train: 1241 samples\n",
      "Validation: 287 samples\n",
      "Test: 382 samples\n",
      "Creating lag features for chennai train set\n",
      "Creating lag features for chennai val set\n",
      "Creating lag features for chennai test set\n",
      "\n",
      "Chennai data loaded:\n",
      "Train: 1224 samples\n",
      "Validation: 283 samples\n",
      "Test: 377 samples\n",
      "Creating lag features for delhi train set\n",
      "Creating lag features for delhi val set\n",
      "Creating lag features for delhi test set\n",
      "\n",
      "Delhi data loaded:\n",
      "Train: 1299 samples\n",
      "Validation: 300 samples\n",
      "Test: 400 samples\n",
      "Creating lag features for hyderabad train set\n",
      "Creating lag features for hyderabad val set\n",
      "Creating lag features for hyderabad test set\n",
      "\n",
      "Hyderabad data loaded:\n",
      "Train: 1222 samples\n",
      "Validation: 282 samples\n",
      "Test: 376 samples\n"
     ]
    }
   ],
   "source": [
    "def create_lag_features(data, target_col='AQI', n_lags=7):\n",
    "    \"\"\"Create lag features for the target column\"\"\"\n",
    "    data = data.sort_values('Date')\n",
    "    for i in range(1, n_lags + 1):\n",
    "        data[f'{target_col}_lag_{i}'] = data[target_col].shift(i)\n",
    "    return data\n",
    "\n",
    "# Cell 2: Load pre-split data for each city\n",
    "processed_dir = '../data/processed'\n",
    "cities = ['bengaluru', 'chennai', 'delhi', 'hyderabad']\n",
    "\n",
    "# Dictionary to store data for each city\n",
    "city_data = {}\n",
    "\n",
    "for city in cities:\n",
    "    city_dir = f'{processed_dir}/{city.lower()}'\n",
    "    city_data[city] = {\n",
    "        'train': pd.read_csv(f'{city_dir}/train.csv'),\n",
    "        'val': pd.read_csv(f'{city_dir}/val.csv'),\n",
    "        'test': pd.read_csv(f'{city_dir}/test.csv')\n",
    "    }\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        city_data[city][split]['Date'] = pd.to_datetime(city_data[city][split]['Date'])\n",
    "        \n",
    "        # Check if lag features exist, if not create them\n",
    "        if 'AQI_lag_1' not in city_data[city][split].columns:\n",
    "            print(f\"Creating lag features for {city} {split} set\")\n",
    "            city_data[city][split] = create_lag_features(city_data[city][split])\n",
    "    \n",
    "    print(f\"\\n{city.title()} data loaded:\")\n",
    "    print(f\"Train: {city_data[city]['train'].shape[0]} samples\")\n",
    "    print(f\"Validation: {city_data[city]['val'].shape[0]} samples\")\n",
    "    print(f\"Test: {city_data[city]['test'].shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, let's load the preprocessed data for each city\n",
    "def load_city_data(city_name):\n",
    "    \"\"\"Load preprocessed data for a specific city\"\"\"\n",
    "    data_dir = \"../data/processed\"\n",
    "    city_data = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = f\"{data_dir}/{city_name}/{split}.csv\"\n",
    "        # print(file_path)\n",
    "        if os.path.exists(file_path):\n",
    "            city_data[split] = pd.read_csv(file_path)\n",
    "            city_data[split]['date'] = pd.to_datetime(city_data[split]['Date'])\n",
    "    \n",
    "    return city_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "device = (\n",
    "    torch.device(\"mps\") \n",
    "    if torch.backends.mps.is_available() \n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        \n",
    "        # Improved CNN architecture\n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Global average pooling\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cnn_data_torch(city_data):\n",
    "    numeric_cols = city_data['train'].select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != 'AQI']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(city_data['train'][numeric_cols])\n",
    "    X_val = scaler.transform(city_data['val'][numeric_cols])\n",
    "    X_test = scaler.transform(city_data['test'][numeric_cols])\n",
    "    \n",
    "    X_train = torch.tensor(X_train[:, :, np.newaxis], dtype=torch.float32).permute(0, 2, 1)\n",
    "    X_val = torch.tensor(X_val[:, :, np.newaxis], dtype=torch.float32).permute(0, 2, 1)\n",
    "    X_test = torch.tensor(X_test[:, :, np.newaxis], dtype=torch.float32).permute(0, 2, 1)\n",
    "    \n",
    "    y_train = torch.tensor(city_data['train']['AQI'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    y_val = torch.tensor(city_data['val']['AQI'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    y_test = torch.tensor(city_data['test']['AQI'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), len(numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_cnn_torch(city_data, city_name, epochs=200, batch_size=32, lr=0.001):\n",
    "    \"\"\"Train and evaluate CNN model with improved training process\"\"\"\n",
    "    print(f\"\\n🔄 Preprocessing data for {city_name}...\")\n",
    "    \n",
    "    # Get numeric columns (excluding AQI)\n",
    "    numeric_cols = city_data['train'].select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['AQI']]\n",
    "    \n",
    "    # Handle missing values in features and target\n",
    "    X_train = city_data['train'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_train = city_data['train']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    X_val = city_data['val'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_val = city_data['val']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    X_test = city_data['test'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_test = city_data['test']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Scale targets (important for neural networks)\n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "    y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train_scaled).unsqueeze(1)\n",
    "    y_train = torch.FloatTensor(y_train_scaled)\n",
    "    X_val = torch.FloatTensor(X_val_scaled).unsqueeze(1)\n",
    "    y_val = torch.FloatTensor(y_val_scaled)\n",
    "    X_test = torch.FloatTensor(X_test_scaled).unsqueeze(1)\n",
    "    y_test = torch.FloatTensor(y_test_scaled)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"🔧 Initializing model...\")\n",
    "    device = torch.device('cpu')\n",
    "    input_len = X_train.shape[2]\n",
    "    model = CNNRegressor(input_channels=1, input_length=input_len).to(device)\n",
    "    \n",
    "    # Use AdamW optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # Use MSE loss\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"🏃 Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_true.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"⚠️ Early stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"📊 Evaluating model...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions\n",
    "    def get_predictions(loader):\n",
    "        preds = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                preds.extend(outputs.cpu().numpy())\n",
    "                true.extend(batch_y.numpy())\n",
    "        return np.array(preds), np.array(true)\n",
    "    \n",
    "    val_preds, val_true = get_predictions(val_loader)\n",
    "    test_preds, test_true = get_predictions(test_loader)\n",
    "    \n",
    "    # Inverse transform predictions and true values\n",
    "    val_preds = target_scaler.inverse_transform(val_preds.reshape(-1, 1)).ravel()\n",
    "    val_true = target_scaler.inverse_transform(val_true.reshape(-1, 1)).ravel()\n",
    "    test_preds = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).ravel()\n",
    "    test_true = target_scaler.inverse_transform(test_true.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def calc_metrics(y_true, y_pred):\n",
    "        return {\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'r2': r2_score(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    val_metrics = calc_metrics(val_true, val_preds)\n",
    "    test_metrics = calc_metrics(test_true, test_preds)\n",
    "    \n",
    "    print(f\"\\n📍 Evaluation Results for {city_name} (PyTorch CNN)\")\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"RMSE: {val_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {val_metrics['mae']:.2f}\")\n",
    "    print(f\"R²: {val_metrics['r2']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"RMSE: {test_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {test_metrics['mae']:.2f}\")\n",
    "    print(f\"R²: {test_metrics['r2']:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'val_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'predictions': {\n",
    "            'val': val_preds,\n",
    "            'test': test_preds\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Preprocessing data for delhi...\n",
      "🔧 Initializing model...\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2640, Val Loss: 0.1059\n",
      "Epoch [20/200], Train Loss: 0.2041, Val Loss: 0.0998\n",
      "Epoch [30/200], Train Loss: 0.1885, Val Loss: 0.0813\n",
      "⚠️ Early stopping at epoch 34\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for delhi (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 30.85\n",
      "MAE: 23.76\n",
      "R²: 0.93\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 32.49\n",
      "MAE: 23.78\n",
      "R²: 0.92\n"
     ]
    }
   ],
   "source": [
    "cnn_torch_results = train_and_evaluate_cnn_torch(city_data['delhi'], 'delhi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_cnn_torch_v2(city_data, city_name, model, epochs=200, batch_size=32, lr=0.001):\n",
    "    \"\"\"Train and evaluate CNN model with improved training process - Version 2 with model parameter support\"\"\"\n",
    "    print(f\"\\n🔄 Preprocessing data for {city_name}...\")\n",
    "    \n",
    "    # Get numeric columns (excluding AQI)\n",
    "    numeric_cols = city_data['train'].select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['AQI']]\n",
    "    \n",
    "    # Handle missing values in features and target\n",
    "    X_train = city_data['train'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_train = city_data['train']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    X_val = city_data['val'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_val = city_data['val']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    X_test = city_data['test'][numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "    y_test = city_data['test']['AQI'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Scale targets (important for neural networks)\n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "    y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "    y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train_scaled).unsqueeze(1)\n",
    "    y_train = torch.FloatTensor(y_train_scaled)\n",
    "    X_val = torch.FloatTensor(X_val_scaled).unsqueeze(1)\n",
    "    y_val = torch.FloatTensor(y_val_scaled)\n",
    "    X_test = torch.FloatTensor(X_test_scaled).unsqueeze(1)\n",
    "    y_test = torch.FloatTensor(y_test_scaled)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"🔧 Initializing model...\")\n",
    "    \n",
    "    print(f\"Using model: {model.__class__.__name__}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Use AdamW optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # Use MSE loss\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"🏃 Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_true.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"⚠️ Early stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"📊 Evaluating model...\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions\n",
    "    def get_predictions(loader):\n",
    "        preds = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                preds.extend(outputs.cpu().numpy())\n",
    "                true.extend(batch_y.numpy())\n",
    "        return np.array(preds), np.array(true)\n",
    "    \n",
    "    val_preds, val_true = get_predictions(val_loader)\n",
    "    test_preds, test_true = get_predictions(test_loader)\n",
    "    \n",
    "    # Inverse transform predictions and true values\n",
    "    val_preds = target_scaler.inverse_transform(val_preds.reshape(-1, 1)).ravel()\n",
    "    val_true = target_scaler.inverse_transform(val_true.reshape(-1, 1)).ravel()\n",
    "    test_preds = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).ravel()\n",
    "    test_true = target_scaler.inverse_transform(test_true.reshape(-1, 1)).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def calc_metrics(y_true, y_pred):\n",
    "        return {\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'r2': r2_score(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    val_metrics = calc_metrics(val_true, val_preds)\n",
    "    test_metrics = calc_metrics(test_true, test_preds)\n",
    "    \n",
    "    print(f\"\\n📍 Evaluation Results for {city_name} (PyTorch CNN)\")\n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"RMSE: {val_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {val_metrics['mae']:.2f}\")\n",
    "    print(f\"R²: {val_metrics['r2']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"RMSE: {test_metrics['rmse']:.2f}\")\n",
    "    print(f\"MAE: {test_metrics['mae']:.2f}\")\n",
    "    print(f\"R²: {test_metrics['r2']:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'val_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'predictions': {\n",
    "            'val': val_preds,\n",
    "            'test': test_preds\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small CNN (our current model as baseline)\n",
    "class SmallCNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(SmallCNNRegressor, self).__init__()\n",
    "        \n",
    "        # Small architecture [32, 64, 128] -> [64, 32, 1]\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "# Medium CNN with more layers and units\n",
    "class MediumCNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(MediumCNNRegressor, self).__init__()\n",
    "        \n",
    "        # Medium architecture [64, 128, 256, 512] -> [256, 128, 64, 1]\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "# Large CNN with even more layers and units\n",
    "class LargeCNNRegressor(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_length=7):\n",
    "        super(LargeCNNRegressor, self).__init__()\n",
    "        \n",
    "        # Large architecture [128, 256, 512, 1024, 2048] -> [1024, 512, 256, 128, 1]\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Conv1d(1024, 2048, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "        \n",
    "\n",
    "def train_model_variant(model_class, model_name, city_data, city_name, **kwargs):\n",
    "    \"\"\"Train a specific model variant\"\"\"\n",
    "    # Default parameters\n",
    "    params = {\n",
    "        'epochs': 200,\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.001\n",
    "    }\n",
    "    params.update(kwargs)\n",
    "    \n",
    "    print(f\"\\n🔄 Training {model_name} for {city_name}...\")\n",
    "    model = model_class(input_channels=1)\n",
    "    results = train_and_evaluate_cnn_torch_v2(\n",
    "        city_data, \n",
    "        city_name,\n",
    "        model=model,\n",
    "        epochs=params['epochs'],\n",
    "        batch_size=params['batch_size'],\n",
    "        lr=params['learning_rate']\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "model_configs = {\n",
    "    'small_cnn': {\n",
    "        'model_class': SmallCNNRegressor,\n",
    "        'params': {\n",
    "            'batch_size': 32,\n",
    "            'epochs': 200,\n",
    "            'learning_rate': 0.001\n",
    "        }\n",
    "    },\n",
    "    'medium_cnn': {\n",
    "        'model_class': MediumCNNRegressor,\n",
    "        'params': {\n",
    "            'batch_size': 64,\n",
    "            'epochs': 200,\n",
    "            'learning_rate': 0.0005\n",
    "        }\n",
    "    },\n",
    "    'large_cnn': {\n",
    "        'model_class': LargeCNNRegressor,\n",
    "        'params': {\n",
    "            'batch_size': 32,\n",
    "            'epochs': 200,\n",
    "            'learning_rate': 0.0005\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for delhi...\n",
      "Loaded train set: 1299 samples\n",
      "Loaded val set: 300 samples\n",
      "Loaded test set: 400 samples\n",
      "Loading data for bengaluru...\n",
      "Loaded train set: 1241 samples\n",
      "Loaded val set: 287 samples\n",
      "Loaded test set: 382 samples\n",
      "Loading data for chennai...\n",
      "Loaded train set: 1224 samples\n",
      "Loaded val set: 283 samples\n",
      "Loaded test set: 377 samples\n",
      "Loading data for hyderabad...\n",
      "Loaded train set: 1222 samples\n",
      "Loaded val set: 282 samples\n",
      "Loaded test set: 376 samples\n"
     ]
    }
   ],
   "source": [
    "def load_city_data(city_name):\n",
    "    \"\"\"Load and prepare data for a specific city\"\"\"\n",
    "    # First, let's search for the data files\n",
    "    print(f\"Loading data for {city_name}...\")\n",
    "    \n",
    "    def create_lag_features(data, target_col='AQI', n_lags=7):\n",
    "        \"\"\"Create lag features for the target column\"\"\"\n",
    "        data = data.sort_values('Date')\n",
    "        for i in range(1, n_lags + 1):\n",
    "            data[f'{target_col}_lag_{i}'] = data[target_col].shift(i)\n",
    "        return data\n",
    "\n",
    "    # Load data from the processed directory\n",
    "    processed_dir = '../data/processed'\n",
    "    city_dir = f'{processed_dir}/{city_name.lower()}'\n",
    "    \n",
    "    city_data = {}\n",
    "    \n",
    "    # Load train, validation, and test sets\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        file_path = f'{city_dir}/{split}.csv'\n",
    "        try:\n",
    "            data = pd.read_csv(file_path)\n",
    "            data['Date'] = pd.to_datetime(data['Date'])\n",
    "            \n",
    "            # Create lag features if they don't exist\n",
    "            if 'AQI_lag_1' not in data.columns:\n",
    "                data = create_lag_features(data)\n",
    "            \n",
    "            city_data[split] = data\n",
    "            print(f\"Loaded {split} set: {len(data)} samples\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find {file_path}\")\n",
    "            return None\n",
    "    \n",
    "    return city_data\n",
    "\n",
    "# Load the data before training models\n",
    "city_data = load_city_data('delhi')\n",
    "city_data = load_city_data('bengaluru')\n",
    "city_data = load_city_data('chennai')\n",
    "city_data = load_city_data('hyderabad')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Training small_cnn for delhi...\n",
      "\n",
      "🔄 Preprocessing data for delhi...\n",
      "🔧 Initializing model...\n",
      "Using model: SmallCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2725, Val Loss: 0.1496\n",
      "Epoch [20/200], Train Loss: 0.2112, Val Loss: 0.0816\n",
      "Epoch [30/200], Train Loss: 0.1863, Val Loss: 0.1001\n",
      "⚠️ Early stopping at epoch 33\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for delhi (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 36.38\n",
      "MAE: 27.75\n",
      "R²: 0.90\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 35.66\n",
      "MAE: 26.49\n",
      "R²: 0.90\n",
      "\n",
      "🔄 Training medium_cnn for delhi...\n",
      "\n",
      "🔄 Preprocessing data for delhi...\n",
      "🔧 Initializing model...\n",
      "Using model: MediumCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2473, Val Loss: 0.1333\n",
      "Epoch [20/200], Train Loss: 0.1790, Val Loss: 0.0906\n",
      "Epoch [30/200], Train Loss: 0.1644, Val Loss: 0.1009\n",
      "⚠️ Early stopping at epoch 37\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for delhi (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 35.05\n",
      "MAE: 27.59\n",
      "R²: 0.91\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 31.78\n",
      "MAE: 24.58\n",
      "R²: 0.92\n",
      "\n",
      "🔄 Training large_cnn for delhi...\n",
      "\n",
      "🔄 Preprocessing data for delhi...\n",
      "🔧 Initializing model...\n",
      "Using model: LargeCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.2585, Val Loss: 0.1568\n",
      "Epoch [20/200], Train Loss: 0.1801, Val Loss: 0.0867\n",
      "⚠️ Early stopping at epoch 22\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for delhi (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 34.89\n",
      "MAE: 27.47\n",
      "R²: 0.91\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 32.32\n",
      "MAE: 25.23\n",
      "R²: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Train all variants\n",
    "results = {}\n",
    "for model_name, config in model_configs.items():\n",
    "    results[model_name] = train_model_variant(\n",
    "        config['model_class'],\n",
    "        model_name,\n",
    "        city_data,\n",
    "        'delhi',\n",
    "        **config['params']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Training small_cnn for bengaluru...\n",
      "\n",
      "🔄 Preprocessing data for bengaluru...\n",
      "🔧 Initializing model...\n",
      "Using model: SmallCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.5075, Val Loss: 0.1078\n",
      "Epoch [20/200], Train Loss: 0.4118, Val Loss: 0.0739\n",
      "Epoch [30/200], Train Loss: 0.3857, Val Loss: 0.0640\n",
      "Epoch [40/200], Train Loss: 0.3019, Val Loss: 0.0638\n",
      "Epoch [50/200], Train Loss: 0.3236, Val Loss: 0.0609\n",
      "Epoch [60/200], Train Loss: 0.2905, Val Loss: 0.0674\n",
      "Epoch [70/200], Train Loss: 0.3362, Val Loss: 0.0637\n",
      "Epoch [80/200], Train Loss: 0.3008, Val Loss: 0.0727\n",
      "⚠️ Early stopping at epoch 90\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for bengaluru (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 14.86\n",
      "MAE: 11.23\n",
      "R²: 0.69\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 16.75\n",
      "MAE: 13.16\n",
      "R²: 0.78\n",
      "\n",
      "🔄 Training medium_cnn for bengaluru...\n",
      "\n",
      "🔄 Preprocessing data for bengaluru...\n",
      "🔧 Initializing model...\n",
      "Using model: MediumCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.4627, Val Loss: 0.0714\n",
      "Epoch [20/200], Train Loss: 0.3843, Val Loss: 0.0611\n",
      "Epoch [30/200], Train Loss: 0.3360, Val Loss: 0.0690\n",
      "Epoch [40/200], Train Loss: 0.3331, Val Loss: 0.0536\n",
      "Epoch [50/200], Train Loss: 0.2988, Val Loss: 0.0528\n",
      "⚠️ Early stopping at epoch 51\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for bengaluru (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 13.72\n",
      "MAE: 10.66\n",
      "R²: 0.73\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 15.81\n",
      "MAE: 12.62\n",
      "R²: 0.80\n",
      "\n",
      "🔄 Training large_cnn for bengaluru...\n",
      "\n",
      "🔄 Preprocessing data for bengaluru...\n",
      "🔧 Initializing model...\n",
      "Using model: LargeCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.4669, Val Loss: 0.2903\n",
      "Epoch [20/200], Train Loss: 0.4114, Val Loss: 0.0561\n",
      "Epoch [30/200], Train Loss: 0.3152, Val Loss: 0.0442\n",
      "Epoch [40/200], Train Loss: 0.4085, Val Loss: 0.0692\n",
      "⚠️ Early stopping at epoch 45\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for bengaluru (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 14.49\n",
      "MAE: 11.57\n",
      "R²: 0.70\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 17.24\n",
      "MAE: 14.27\n",
      "R²: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Train all variants\n",
    "results = {}\n",
    "for model_name, config in model_configs.items():\n",
    "    results[model_name] = train_model_variant(\n",
    "        config['model_class'],\n",
    "        model_name,\n",
    "        city_data,\n",
    "        'bengaluru',\n",
    "        **config['params']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Training small_cnn for chennai...\n",
      "\n",
      "🔄 Preprocessing data for chennai...\n",
      "🔧 Initializing model...\n",
      "Using model: SmallCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.5495, Val Loss: 0.1582\n",
      "Epoch [20/200], Train Loss: 0.4220, Val Loss: 0.0642\n",
      "Epoch [30/200], Train Loss: 0.2935, Val Loss: 0.0779\n",
      "⚠️ Early stopping at epoch 40\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for chennai (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 14.92\n",
      "MAE: 11.57\n",
      "R²: 0.68\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 16.73\n",
      "MAE: 13.11\n",
      "R²: 0.78\n",
      "\n",
      "🔄 Training medium_cnn for chennai...\n",
      "\n",
      "🔄 Preprocessing data for chennai...\n",
      "🔧 Initializing model...\n",
      "Using model: MediumCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.4508, Val Loss: 0.0827\n",
      "Epoch [20/200], Train Loss: 0.3624, Val Loss: 0.0568\n",
      "Epoch [30/200], Train Loss: 0.3611, Val Loss: 0.0608\n",
      "Epoch [40/200], Train Loss: 0.3165, Val Loss: 0.0466\n",
      "⚠️ Early stopping at epoch 42\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for chennai (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 12.87\n",
      "MAE: 9.82\n",
      "R²: 0.77\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 15.16\n",
      "MAE: 11.96\n",
      "R²: 0.82\n",
      "\n",
      "🔄 Training large_cnn for chennai...\n",
      "\n",
      "🔄 Preprocessing data for chennai...\n",
      "🔧 Initializing model...\n",
      "Using model: LargeCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.5346, Val Loss: 0.2152\n",
      "Epoch [20/200], Train Loss: 0.3985, Val Loss: 0.0790\n",
      "Epoch [30/200], Train Loss: 0.2765, Val Loss: 0.0813\n",
      "Epoch [40/200], Train Loss: 0.2367, Val Loss: 0.0628\n",
      "⚠️ Early stopping at epoch 42\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for chennai (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 14.18\n",
      "MAE: 11.12\n",
      "R²: 0.71\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 15.17\n",
      "MAE: 12.35\n",
      "R²: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Train all variants\n",
    "results = {}\n",
    "for model_name, config in model_configs.items():\n",
    "    results[model_name] = train_model_variant(\n",
    "        config['model_class'],\n",
    "        model_name,\n",
    "        city_data,\n",
    "        'chennai',\n",
    "        **config['params']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Training small_cnn for hyderabad...\n",
      "\n",
      "🔄 Preprocessing data for hyderabad...\n",
      "🔧 Initializing model...\n",
      "Using model: SmallCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.4975, Val Loss: 0.0932\n",
      "Epoch [20/200], Train Loss: 0.3771, Val Loss: 0.1083\n",
      "Epoch [30/200], Train Loss: 0.3117, Val Loss: 0.0679\n",
      "Epoch [40/200], Train Loss: 0.3001, Val Loss: 0.0759\n",
      "Epoch [50/200], Train Loss: 0.2758, Val Loss: 0.0992\n",
      "⚠️ Early stopping at epoch 60\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for hyderabad (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 13.90\n",
      "MAE: 10.57\n",
      "R²: 0.73\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 16.03\n",
      "MAE: 12.61\n",
      "R²: 0.80\n",
      "\n",
      "🔄 Training medium_cnn for hyderabad...\n",
      "\n",
      "🔄 Preprocessing data for hyderabad...\n",
      "🔧 Initializing model...\n",
      "Using model: MediumCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.4540, Val Loss: 0.0839\n",
      "Epoch [20/200], Train Loss: 0.3427, Val Loss: 0.1029\n",
      "Epoch [30/200], Train Loss: 0.3329, Val Loss: 0.0556\n",
      "Epoch [40/200], Train Loss: 0.3298, Val Loss: 0.0522\n",
      "Epoch [50/200], Train Loss: 0.3129, Val Loss: 0.0495\n",
      "Epoch [60/200], Train Loss: 0.2439, Val Loss: 0.0479\n",
      "⚠️ Early stopping at epoch 68\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for hyderabad (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 12.91\n",
      "MAE: 9.94\n",
      "R²: 0.76\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 16.36\n",
      "MAE: 13.23\n",
      "R²: 0.79\n",
      "\n",
      "🔄 Training large_cnn for hyderabad...\n",
      "\n",
      "🔄 Preprocessing data for hyderabad...\n",
      "🔧 Initializing model...\n",
      "Using model: LargeCNNRegressor\n",
      "🏃 Starting training...\n",
      "Epoch [10/200], Train Loss: 0.4619, Val Loss: 0.7787\n",
      "Epoch [20/200], Train Loss: 0.4375, Val Loss: 0.0475\n",
      "Epoch [30/200], Train Loss: 0.3097, Val Loss: 0.0651\n",
      "⚠️ Early stopping at epoch 37\n",
      "📊 Evaluating model...\n",
      "\n",
      "📍 Evaluation Results for hyderabad (PyTorch CNN)\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 13.40\n",
      "MAE: 10.56\n",
      "R²: 0.75\n",
      "\n",
      "Test Metrics:\n",
      "RMSE: 14.89\n",
      "MAE: 11.94\n",
      "R²: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Train all variants\n",
    "results = {}\n",
    "for model_name, config in model_configs.items():\n",
    "    results[model_name] = train_model_variant(\n",
    "        config['model_class'],\n",
    "        model_name,\n",
    "        city_data,\n",
    "        'hyderabad',\n",
    "        **config['params']\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
